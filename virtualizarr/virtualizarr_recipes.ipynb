{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180973dc-a4fc-43aa-bbc5-d56ce3e9edba",
   "metadata": {},
   "source": [
    "# Virtualizarr Useful Recipes with NASA Earthdata\n",
    "\n",
    "#### *Author: Dean Henze, PO.DAAC*\n",
    "\n",
    "*Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not constitute or imply its endorsement by the United States Government or the Jet Propulsion Laboratory, California Institute of Technology.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e719d0-17d1-45e9-9933-cb49a7cef7a9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook goes through several functionalities of the virtualizarr package to create virtual reference files, specifically using it with NASA Earthdata and utilizing the `earthaccess` package. It is meant to be a quick-start reference that introduces some key capabilities / characteristics of the package once a user has a high-level understanding of virtual data sets and the cloud-computing challenges it addresses (see references in the *Prerequisite knowledge* section below). In short, virtualizarr is a Python package to create \"reference files\", which can be thought of as road maps for the computer to efficiently navigate through large arrays in an Earthdata file (or any file). Once a reference file for a data set is created, it can be accessed and used to e.g. to lazy load data faster, access subsets of the data quicker (spatially, temporally, or any other dimension in the data set), and in some cases perform computations faster.\n",
    "\n",
    "The functionalities of virtualizarr covered in this notebook are:\n",
    "\n",
    "1. **Getting Data File endpoints in Earthdata Cloud**\n",
    "2. **Generating reference files for 1 day, 1 year, and the entire record of a ~750 GB data set**. The data set, available on PO.DAAC, is the Level 4 global gridded 6-hourly wind product from the Cross-Calibrated Multi-Platform project (https://doi.org/10.5067/CCMP-6HW10M-L4V31). This section also covers speeding up the reference file creations using parallel computing. Reference files are saved in both JSON and PARQUET formats. The latter is an important format as it reduces the reference file size by ~30x in our tests.\n",
    "3. **Combining reference files**. The ability to combine reference files together rather than having to create the combined product from scratch is important since it can save computing resources/time. This notebooks explores (3.1) Adding an extra day of the CCMP record to the year-long reference file created in Section 2, and (3.2) combing 2 year-long reference files together.\n",
    "4. **Using the reference file to perform a basic analysis on the CCMP data set with a parallel computing cluster.** Parallel computing on both a local and distributed cluster are tested. For the local cluster, we are able to run all computations successfully. For the distributed cluster, we are only able to run computations if the reference file is first loaded fully into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce1593-e158-44d4-aab8-9f91027a19ba",
   "metadata": {},
   "source": [
    "## Requirements, prerequisite knowledge, learning outcomes\n",
    "\n",
    "#### Requirements to run this notebook\n",
    "\n",
    "* Earthdata login account: An Earthdata Login account is required to access data from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account.\n",
    "\n",
    "* Compute environment: This notebook is meant to be run in the cloud (AWS instance running in us-west-2). We used an `m6i.4xlarge` EC2 instance (16 CPU's, 64 GiB memory) to complete Section 4 on parallel computing, although this is likely overkill for the other sections. At minimum we recommend a VM with 10 CPU's to make the parallel computations in Section 2.2.1 faster.\n",
    "\n",
    "* Optional Coiled account: To run the optional sections on distributed clusters, Create a coiled account (free to sign up), and connect it to an AWS account. For more information on Coiled, setting up an account, and connecting it to an AWS account, see their website [https://www.coiled.io](https://www.coiled.io). \n",
    "\n",
    "#### Prerequisite knowledge\n",
    "\n",
    "* This notebook covers kerchunk functionality but does not present the high-level ideas behind it. For an understanding of reference files and how they are meant to enhance in-cloud access to file formats that are not cloud optimized (such netCDF, HDF), please see e.g. this [kerchunk page](https://fsspec.github.io/kerchunk/), or [this page on virtualizarr](https://virtualizarr.readthedocs.io/en/latest/) (a package with similar functionality).\n",
    "\n",
    "* Familiarity with the `earthaccess` and `Xarray` packages. Familiarity with directly accessing NASA Earthdata in the cloud. \n",
    "\n",
    "* The Cookbook notebook on [Dask basics](https://podaac.github.io/tutorials/notebooks/Advanced_cloud/basic_dask.html) is handy for those new to parallel computating and wanting to implement it in Sections 1.2.1 and 3.2.\n",
    "\n",
    "#### Learning Outcomes\n",
    "\n",
    "This notebook demonstrates several recipes for key kerchunk functionalities with NASA Earthdata. It is meant to be used after the user has a high level understanding of kerchunk and the challenges it is trying to solve, at which point this notebook: \n",
    "\n",
    "* Demonstrates how to implement the package,\n",
    "* Highlights several characteristics of the package which will likely be of interest for utilizing it with Earthdata in common workflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f65dd1-39f6-480a-aa63-adbbd9863e8f",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "#### ***Note Zarr Version***\n",
    "***Zarr version 2 is needed for the current implementation of this notebook, due to (as of February 2025) Zarr version 3 not accepting `FSMap` objects.***\n",
    "\n",
    "We ran this notebook in a Python 3.12 environment. The minimal working install we used to run this notebook from a clean environment was:\n",
    "```\n",
    "pip install zarr==2.18.4 fastparquet==2024.5.0 xarray==2024.1.0 earthaccess==0.11.0 fsspec==2024.10.0 \"dask[complete]\"==2024.5.2 h5netcdf==1.3.0 ujson==5.10.0 matplotlib==3.9.2 jupyterlab jupyter-server-proxy virtualizarr==1.3.0\n",
    "```\n",
    "And optionally:\n",
    "```\n",
    "pip install coiled==1.58.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0b1c0c-c8f9-412c-8038-4b674de896c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in packages\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Filesystem management \n",
    "import fsspec\n",
    "import earthaccess\n",
    "\n",
    "# Data analysis\n",
    "import xarray as xr\n",
    "from virtualizarr import open_virtual_dataset\n",
    "\n",
    "# Parallel computing \n",
    "import multiprocessing\n",
    "from dask import delayed\n",
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Other\n",
    "import ujson\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e601488-7348-45cb-bfed-068121fc4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e58626-85f4-4fed-b5af-04736ca6f83d",
   "metadata": {},
   "source": [
    "## Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c5a123-6025-4a85-a7b0-4b9b747a9a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x7f4516ec0380>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.set_options( # display options for xarray objects\n",
    "    display_expand_attrs=False,\n",
    "    display_expand_coords=True,\n",
    "    display_expand_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6901e7c4-66ca-4dfb-bd8f-aaf2f0291764",
   "metadata": {},
   "source": [
    "## 1. Get Data File S3 endpoints in Earthdata Cloud \n",
    "The first step is to find the S3 endpoints to the files. Handling access credentials to Earthdata and then finding the endpoints can be done a number of ways (e.g. using the `requests`, `s3fs` packages) but we use the `earthaccess` package for its ease of use. We get the endpoints for all files in the CCMP record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20dbc070-d5f7-407e-b92e-4fda1b8a82ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<earthaccess.auth.Auth at 0x7f45c8db2690>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Earthdata creds\n",
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7519aab1-b2aa-40fa-862a-62ed69439ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AWS creds. Note that if you spend more than 1 hour in the notebook, you may have to re-run this line!!!\n",
    "fs = earthaccess.get_s3_filesystem(daac=\"PODAAC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487b4dd6-39c6-4d7e-8051-eddcd22e2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate CCMP file information / metadata:\n",
    "granule_info = earthaccess.search_data(\n",
    "    short_name=\"CCMP_WINDS_10M6HR_L4_V3.1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc311a3-df1a-4565-a314-658bff759203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_19930102_V03.1_L4.nc',\n",
       " 's3://podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_19930103_V03.1_L4.nc',\n",
       " 's3://podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_19930105_V03.1_L4.nc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get S3 endpoints for all files:\n",
    "data_s3links = [g.data_links(access=\"direct\")[0] for g in granule_info]\n",
    "data_s3links[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e33a1a-bd28-4b60-b411-18e25c7cb1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_19930102_V03.1_L4.nc',\n",
       " 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_19930103_V03.1_L4.nc',\n",
       " 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/CCMP_WINDS_10M6HR_L4_V3.1/CCMP_Wind_Analysis_19930105_V03.1_L4.nc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get HTTPS endpoints for all files:\n",
    "data_httpslinks = [g.data_links(access=\"external\")[0] for g in granule_info]\n",
    "data_httpslinks[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20756ad2-4fdf-4f7a-8582-aa7d59ea35e5",
   "metadata": {},
   "source": [
    "## 2. Generate reference files for 1 day, 1 year, and entire record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146d00a-cf61-4ed4-96e2-6db0a19bb466",
   "metadata": {},
   "source": [
    "### 2.1 First day\n",
    "The virtualizarr function to generate reference information is pretty compact. We use it on one file for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329db221-b2dc-41de-8e67-3405402e4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 66MB\n",
      "Dimensions:    (time: 4, latitude: 720, longitude: 1440)\n",
      "Coordinates:\n",
      "    longitude  (longitude) float32 6kB ManifestArray<shape=(1440,), dtype=flo...\n",
      "    time       (time) float32 16B ManifestArray<shape=(4,), dtype=float32, ch...\n",
      "    latitude   (latitude) float32 3kB ManifestArray<shape=(720,), dtype=float...\n",
      "Data variables:\n",
      "    ws         (time, latitude, longitude) float32 17MB ManifestArray<shape=(...\n",
      "    uwnd       (time, latitude, longitude) float32 17MB ManifestArray<shape=(...\n",
      "    nobs       (time, latitude, longitude) float32 17MB ManifestArray<shape=(...\n",
      "    vwnd       (time, latitude, longitude) float32 17MB ManifestArray<shape=(...\n",
      "Attributes: (54)\n",
      "CPU times: user 326 ms, sys: 74.5 ms, total: 401 ms\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reader_opts = {\"storage_options\": fs.storage_options} # S3 filesystem creds from previous section.\n",
    "virtual_ds_example = open_virtual_dataset(data_s3links[0], indexes={}, reader_options=reader_opts)\n",
    "print(virtual_ds_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b850da-5df5-460e-81f3-0450a2904667",
   "metadata": {},
   "source": [
    "The reference can be saved to file and used to open the corresponding CCMP data file with Xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e53d64-6547-4193-a24e-6018bbad0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "virtual_ds_example.virtualize.to_kerchunk('virtual_ds_example.json', format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15707ccc-fcb3-4bf9-94cb-71b11d468054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data using the reference file, using a small wrapper function around xarray's open_dataset. \n",
    "# This will shorten code blocks in other sections. \n",
    "def opendf_withref(ref, fs_data):\n",
    "    \"\"\"\n",
    "    \"ref\" is a reference file or object. \"fs_data\" is a filesystem with credentials to\n",
    "    access the actual data files. \n",
    "    \"\"\"\n",
    "    storage_opts = {\"fo\": ref, \"remote_protocol\": \"s3\", \"remote_options\": fs_data.storage_options}\n",
    "    fs_ref = fsspec.filesystem('reference', **storage_opts)\n",
    "    m = fs_ref.get_mapper('')\n",
    "    data = xr.open_dataset(\n",
    "        m, engine=\"zarr\", chunks={},\n",
    "        backend_kwargs={\"consolidated\": False}\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b02a2f1c-2b53-4a0b-ba0e-2bc0a35fce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 66MB\n",
      "Dimensions:    (latitude: 720, longitude: 1440, time: 4)\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n",
      "  * longitude  (longitude) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time       (time) datetime64[ns] 32B 1993-01-02 ... 1993-01-02T18:00:00\n",
      "Data variables:\n",
      "    nobs       (time, latitude, longitude) float32 17MB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    uwnd       (time, latitude, longitude) float32 17MB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    vwnd       (time, latitude, longitude) float32 17MB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    ws         (time, latitude, longitude) float32 17MB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "Attributes: (54)\n"
     ]
    }
   ],
   "source": [
    "data_example = opendf_withref('virtual_ds_example.json', fs)\n",
    "print(data_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051e7d34-594c-40dc-9219-6e281ccea777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 bytes\n"
     ]
    }
   ],
   "source": [
    "# Also useful to note, these reference objects don't take much memory:\n",
    "print(sys.getsizeof(virtual_ds_example), \"bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b4028-b31d-4c28-a78d-2fedebcf967c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2 First year\n",
    "Reference information for each data file in the year is created individually, and then the combined reference file for the year can be created.\n",
    "\n",
    "For us, reference file creation for a single file takes about 0.7 seconds, so processing a year of files would take about 4.25 minuts. One can easly accomplish this with a for-loop:\n",
    "\n",
    "```\n",
    "virtual_ds_list = [\n",
    "    open_virtual_dataset(p, indexes={}, reader_options={\"storage_options\": fs.storage_options})\n",
    "    for p in data_s3links\n",
    "    ]\n",
    "```\n",
    "\n",
    "However, we speed things up using basic parallel computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4fc53c-2bf2-4b9d-9ab7-32a498011bd4",
   "metadata": {},
   "source": [
    "### 2.2.1 Method 1: parallelize using Dask local cluster\n",
    "If using the suggested `m6i.8xlarge` AWS EC2 instance, there are 32 CPUs available and each should have enough memory that we can utilize all 32 at once. If working on a different VM-type, change the `n_workers` in the call to `Client()` below as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6bae28c-0527-4ad8-8065-f6187fb46961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count = 16\n"
     ]
    }
   ],
   "source": [
    "# Check how many cpu's are on this VM:\n",
    "print(\"CPU count =\", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "081fdd0e-03c7-4746-9066-9c1342b99cb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalCluster(e5a9615d, 'tcp://127.0.0.1:42901', workers=15, threads=15, memory=60.83 GiB)\n",
      "View any work being done on the cluster here https://cluster-gjspj.dask.host/jupyter/proxy/8787/status\n"
     ]
    }
   ],
   "source": [
    "# Start up cluster and print some information about it:\n",
    "client = Client(n_workers=15, threads_per_worker=1)\n",
    "print(client.cluster)\n",
    "print(\"View any work being done on the cluster here\", client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd722c4e-7b51-4807-b138-c9e1052ba465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.76 s, sys: 685 ms, total: 4.45 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create individual references:\n",
    "open_vds_par = delayed(open_virtual_dataset)\n",
    "tasks = [open_vds_par(p, indexes={}, reader_options=reader_opts) for p in data_s3links[:365]]\n",
    "virtual_ds_list = list(da.compute(*tasks)) # The xr.combine_nested() function below needs a list rather than a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e17f4-8a71-49ea-b18c-de13f2cebd2a",
   "metadata": {},
   "source": [
    "Using the individual references to create the combined reference is fast and does not requre parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45722c2f-a4c5-4249-a2d6-06761ff35052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 0 ns, total: 128 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the combined reference\n",
    "virtual_ds_combined = xr.combine_nested(virtual_ds_list, concat_dim='time', coords='minimal', compat='override', combine_attrs='drop_conflicts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1628e4a-0e46-45ec-bd9e-63922fd085f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in JSON or PARQUET format:\n",
    "fname_combined_json = 'ref_combined_1year.json'\n",
    "fname_combined_parq = 'ref_combined_1year.parq'\n",
    "virtual_ds_combined.virtualize.to_kerchunk(fname_combined_json, format='json')\n",
    "virtual_ds_combined.virtualize.to_kerchunk(fname_combined_parq, format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bfee1-087a-4290-83a3-609de26a519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test lazy loading of the combine reference file JSON:\n",
    "data_json = opendf_withref(fname_combined_json, fs)\n",
    "print(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89451f0c-7453-4fee-843f-5faa196bcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test lazy loading of the combine reference file PARQUET:\n",
    "data_parq = opendf_withref(fname_combined_parq, fs)\n",
    "print(data_parq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083114d-de86-4860-911f-f44e21877725",
   "metadata": {},
   "source": [
    "### 2.2.2 Optional method 2: parallelize using distributed cluster with Coiled\n",
    "At PO.DAAC we have been testing the third party software/package Coiled which makes it easy to spin up distributed computing clusters in the cloud. Since we suspect that Coiled may become a key member of the Cloud ecosystem for earth science researchers, this optional section is included, which can be used as an alternative to Section 2.2.1 for generating the individual reference files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fa76bec-1ef7-4ed1-bc58-f16eea3aa9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fda0faacb114fd39803e45b29207d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dcee6c068c4302ac3e1b5ece596536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.6 s, sys: 653 ms, total: 8.26 s\n",
      "Wall time: 8min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## --------------------------------------------\n",
    "## Create single reference files with parallel computing using Coiled\n",
    "## --------------------------------------------\n",
    "\n",
    "# Wrap `open_virtual_dataset()` into coiled function and copy to mulitple VM's:\n",
    "open_vds_par = coiled.function(\n",
    "    region=\"us-west-2\", spot_policy=\"on-demand\", \n",
    "    vm_type=\"m6i.large\", n_workers=30\n",
    "    )(open_virtual_dataset)\n",
    "\n",
    "# Begin computations:\n",
    "results = open_vds_par.map(data_s3links[:], indexes={}, reader_options=reader_opts)\n",
    "\n",
    "virtual_ds_list = []\n",
    "for r in results:\n",
    "    virtual_ds_list.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d0c311a-9523-44d4-96eb-b88db5bda632",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_vds_par.cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d5580-472d-4525-994f-32e96bf85293",
   "metadata": {},
   "source": [
    "Using the individual references to create the combined reference is fast and does not requre parallel computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca199351-8ea0-47a5-99e7-483bc8c75f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.48 s, sys: 33.2 ms, total: 5.51 s\n",
      "Wall time: 5.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combining the individual references works the same as in Section 2.2.1:\n",
    "virtual_ds_combined = xr.combine_nested(virtual_ds_list, concat_dim='time', coords='minimal', compat='override', combine_attrs='drop_conflicts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcc631c2-4747-46ce-866f-e406ca298aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in JSON or PARQUET format:\n",
    "fname_combined_json = 'ref_combined_1year.json'\n",
    "fname_combined_parq = 'ref_combined_1year.parq'\n",
    "virtual_ds_combined.virtualize.to_kerchunk(fname_combined_json, format='json')\n",
    "virtual_ds_combined.virtualize.to_kerchunk(fname_combined_parq, format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af082915-cb86-4e9d-8800-20287f8528dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 775GB\n",
      "Dimensions:    (latitude: 720, longitude: 1440, time: 46696)\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n",
      "  * longitude  (longitude) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time       (time) datetime64[ns] 374kB 1993-01-02 ... 2024-12-31T18:00:00\n",
      "Data variables:\n",
      "    nobs       (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    uwnd       (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    vwnd       (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    ws         (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "Attributes: (44)\n",
      "CPU times: user 37.7 s, sys: 492 ms, total: 38.2 s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test lazy loading of the combine reference file JSON:\n",
    "data_json = opendf_withref(fname_combined_json, fs)\n",
    "print(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38e315d3-bd54-4c94-b993-33c69ab8fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 775GB\n",
      "Dimensions:    (latitude: 720, longitude: 1440, time: 46696)\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n",
      "  * longitude  (longitude) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time       (time) datetime64[ns] 374kB 1993-01-02 ... 2024-12-31T18:00:00\n",
      "Data variables:\n",
      "    nobs       (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    uwnd       (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    vwnd       (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    ws         (time, latitude, longitude) float32 194GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "Attributes: (44)\n",
      "CPU times: user 38.3 s, sys: 351 ms, total: 38.7 s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test lazy loading of the combine reference file PARQUET:\n",
    "data_parq = opendf_withref(fname_combined_parq, fs)\n",
    "print(data_parq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b66a25-1e90-40d1-a600-fea2f47193e9",
   "metadata": {},
   "source": [
    "## 2.3 Entire record\n",
    "\n",
    "Processing the entire record follows the exact same workflow as processing the first year Section 2.2 (either parallelization method). The only modification required is to change the one instance of \n",
    "```\n",
    "data_s3links[:365]\n",
    "```\n",
    "with \n",
    "```\n",
    "data_s3links[:]\n",
    "```\n",
    "when setting up the parallel computations (occurs once in each of Sections 2.2.1 and 2.2.2). Optionally, also change the saved file names e.g. from `ref_combined_1year.json` to `ref_combined_record.json`.\n",
    "\n",
    "For us, processing the entire record using a local cluster on an `m6i.4xlarge` EC2 instance, with 15 workers, took about 13 minutes. Using 30 `m6i.large` VM's on a distributed cluster with Coiled took ~8.5 minutes and cost ~$0.40.\n",
    "\n",
    "Because the virtualizarr package is so efficient at combining many individual reference files together, and because the individual references have such small in-memory requirements, the workflows in Section 2.2 are assumed to scale to tens of thousands of files and TB's of data. However, this assumption will be tested as the techniques in the notebook are applied to progressively larger data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176122c-e85b-4bf1-aa70-804f50d396bb",
   "metadata": {},
   "source": [
    "For us, lazy loading the entire recrod took ~3 minutes. Current work is looking into reducing this time further (expectations were high coming from kerchunk reference file generation). However, 3 minutes is still a massive upgrade - for example, compare that to an attempt at opening these same files with `Xarray` the \"traditional\" way with a call to `xr.open_mfdataset()`. On a smaller machine, the following line of code will either fail or take a long (possibly very long) amount of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec7ac19b-217b-472b-993a-0c560a228c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can try un-commenting and running this but your notebook will probably stall or crash:\n",
    "# fobjs = earthaccess.open(granule_info)\n",
    "# data = xr.open_mfdataset(fobjs[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605954f8-1a25-4945-bccd-b3c0d40af29b",
   "metadata": {},
   "source": [
    "## 3. Combining reference files\n",
    "This section demonstrates that reference files can be combined in two examples:\n",
    "\n",
    "1. A single reference file is appended to the year-long combined reference file generated in the previous section.\n",
    "2. A second year-long combined reference file is created and combined with the first year-long reference file.\n",
    "\n",
    "In both cases, creating the final product (e.g. combining two reference files) is easy and works, ***but currently only works for JSON*** (and the future ice chunk format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30a37e94-5b90-4902-9be1-c76db23d774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case this notebook has been running over an hour, refresh the file system and credentials:\n",
    "fs = earthaccess.get_s3_filesystem(daac=\"PODAAC\")\n",
    "reader_opts = {\"storage_options\": fs.storage_options}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525766d-d62d-4de2-a7b0-1c460aaa001d",
   "metadata": {},
   "source": [
    "### 3.1 Adding an extra day of the CCMP record to the year-long reference file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1f747-3e64-4151-8e3c-f62f511d5b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3440c0c-e9de-4fab-85e7-cbae7ad20e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51b0736a-e581-4e7f-991a-6a2ced3e3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 220 ms, sys: 58.7 ms, total: 279 ms\n",
      "Wall time: 765 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create reference file for 366th CCMP file:\n",
    "vds_extraday = open_virtual_dataset(data_s3links[366], indexes={}, reader_options=reader_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19a23add-33df-4928-8f25-b425985fe81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add it to the year-long reference, then save as new reference file\n",
    "#ref_year1 = open_virtual_dataset('ref_combined_1year.parq', filetype='parquet')\n",
    "vds_year1 = open_virtual_dataset('ref_combined_1year.json', filetype='kerchunk')\n",
    "vds_appended = xr.combine_nested([vds_year1, vds_extraday], concat_dim='time', coords='minimal', compat='override', combine_attrs='drop_conflicts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d28b59ab-7d4b-4b5c-8c47-558837f27054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "1464\n"
     ]
    }
   ],
   "source": [
    "## Confirm time dimension has increased in new reference:\n",
    "print(len(vds_year1['time']))\n",
    "print(len(vds_appended['time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd59aa08-a5f2-40bc-bdd8-07e8c6e782b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 24GB\n",
      "Dimensions:    (latitude: 720, longitude: 1440, time: 1464)\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n",
      "  * longitude  (longitude) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time       (time) datetime64[ns] 12kB 1993-01-02 ... 1994-01-06T18:00:00\n",
      "Data variables:\n",
      "    nobs       (time, latitude, longitude) float32 6GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    uwnd       (time, latitude, longitude) float32 6GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    vwnd       (time, latitude, longitude) float32 6GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    ws         (time, latitude, longitude) float32 6GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "Attributes: (54)\n",
      "CPU times: user 842 ms, sys: 31.8 ms, total: 874 ms\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save to file then use to open new data set\n",
    "vds_appended.virtualize.to_kerchunk('ref_appended.json', format='json')\n",
    "data_appended = opendf_withref('ref_appended.json', fs)\n",
    "print(data_appended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af585b0b-9c7e-45dd-9ea0-8b39f83af375",
   "metadata": {},
   "source": [
    "### 3.2 Combining two year-long combined reference files\n",
    "Individual files for the second year of CCMP are created and combined into a single reference file, then this file is combined with the first year-long reference file. As before, parallel computing is used to speed up creation of the files, but this could also be accomplished with a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f7a3a-511e-4bde-8385-121854129842",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !!!!!!!!!!!\n",
    "## This line only needs to be un-commented and run if you don't have a cluster already running\n",
    "## from Section 2.2.1\n",
    "## !!!!!!!!!!!\n",
    "\n",
    "## Start up cluster:\n",
    "#client = Client(n_workers=15, threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "238e9c31-af8a-4cf8-9c9b-8f687978d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.31 s, sys: 792 ms, total: 5.11 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create individual references for second year:\n",
    "open_vds_par = delayed(open_virtual_dataset)\n",
    "tasks = [open_vds_par(p, indexes={}, reader_options=reader_opts) for p in data_s3links[366:730]]\n",
    "virtual_ds_list_year2 = list(da.compute(*tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ffb9967c-e962-4524-91b9-6eb9845e0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 126 ms, sys: 0 ns, total: 126 ms\n",
      "Wall time: 124 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=\"dark\"],\n",
       "html[data-theme=\"dark\"],\n",
       "body[data-theme=\"dark\"],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1f1f1f;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "  height: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: \"►\";\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: \"▼\";\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: \"(\";\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: \")\";\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: \",\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 24GB\n",
       "Dimensions:    (time: 1456, latitude: 720, longitude: 1440)\n",
       "Coordinates:\n",
       "    latitude   (latitude) float32 3kB ManifestArray&lt;shape=(720,), dtype=float...\n",
       "    time       (time) float32 6kB ManifestArray&lt;shape=(1456,), dtype=float32,...\n",
       "    longitude  (longitude) float32 6kB ManifestArray&lt;shape=(1440,), dtype=flo...\n",
       "Data variables:\n",
       "    ws         (time, latitude, longitude) float32 6GB ManifestArray&lt;shape=(1...\n",
       "    nobs       (time, latitude, longitude) float32 6GB ManifestArray&lt;shape=(1...\n",
       "    uwnd       (time, latitude, longitude) float32 6GB ManifestArray&lt;shape=(1...\n",
       "    vwnd       (time, latitude, longitude) float32 6GB ManifestArray&lt;shape=(1...\n",
       "Attributes: (47)</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-1cd4bf46-8706-4e6d-9a32-167caef23022' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-1cd4bf46-8706-4e6d-9a32-167caef23022' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span>time</span>: 1456</li><li><span>latitude</span>: 720</li><li><span>longitude</span>: 1440</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-12bc4203-69ac-4c7f-b66b-c3f072b80b9e' class='xr-section-summary-in' type='checkbox'  checked><label for='section-12bc4203-69ac-4c7f-b66b-c3f072b80b9e' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>ManifestArray&lt;shape=(720,), dtyp...</div><input id='attrs-61d8b18c-276a-4479-86ce-acf170f681ed' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-61d8b18c-276a-4479-86ce-acf170f681ed' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9c027259-6b4d-4979-a956-816345d52c6c' class='xr-var-data-in' type='checkbox'><label for='data-9c027259-6b4d-4979-a956-816345d52c6c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>_CoordinateAxisType :</span></dt><dd>Lat</dd><dt><span>actual_range :</span></dt><dd>[-78.3499984741211, 78.3499984741211]</dd><dt><span>coordinate_defines :</span></dt><dd>center</dd><dt><span>coverage_content_type :</span></dt><dd>coordinate</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>valid_max :</span></dt><dd>90.0</dd><dt><span>valid_min :</span></dt><dd>-90.0</dd></dl></div><div class='xr-var-data'><pre>ManifestArray&lt;shape=(720,), dtype=float32, chunks=(720,)&gt;</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>ManifestArray&lt;shape=(1456,), dty...</div><input id='attrs-20fedc2a-17d4-42c4-b78c-7cf370ba88ab' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-20fedc2a-17d4-42c4-b78c-7cf370ba88ab' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1c5eac31-0d3b-410b-bff0-3e8706373a77' class='xr-var-data-in' type='checkbox'><label for='data-1c5eac31-0d3b-410b-bff0-3e8706373a77' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>_CoordinateAxisType :</span></dt><dd>Time</dd><dt><span>coverage_content_type :</span></dt><dd>coordinate</dd><dt><span>delta_t :</span></dt><dd>0000-00-00 06:00:00</dd><dt><span>long_name :</span></dt><dd>time of analysis</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>units :</span></dt><dd>hours since 1987-01-01 00:00:00</dd></dl></div><div class='xr-var-data'><pre>ManifestArray&lt;shape=(1456,), dtype=float32, chunks=(4,)&gt;</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>ManifestArray&lt;shape=(1440,), dty...</div><input id='attrs-f3bc5a79-a3f3-45af-943e-9835d5a0bdf2' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-f3bc5a79-a3f3-45af-943e-9835d5a0bdf2' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-bad025cd-7cb3-4355-b369-53403d1627b1' class='xr-var-data-in' type='checkbox'><label for='data-bad025cd-7cb3-4355-b369-53403d1627b1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>_CoordinateAxisType :</span></dt><dd>Lon</dd><dt><span>coordinate_defines :</span></dt><dd>center</dd><dt><span>coverage_content_type :</span></dt><dd>coordinate</dd><dt><span>long_name :</span></dt><dd>longitude</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>valid_max :</span></dt><dd>360.0</dd><dt><span>valid_min :</span></dt><dd>-0.0</dd></dl></div><div class='xr-var-data'><pre>ManifestArray&lt;shape=(1440,), dtype=float32, chunks=(1440,)&gt;</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-baf82ee3-3d3d-41a9-ac2c-cffefc92a615' class='xr-section-summary-in' type='checkbox'  checked><label for='section-baf82ee3-3d3d-41a9-ac2c-cffefc92a615' class='xr-section-summary' >Data variables: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>ws</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>ManifestArray&lt;shape=(1456, 720, ...</div><input id='attrs-24153a8e-976a-48d4-b9e6-b6b55a200739' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-24153a8e-976a-48d4-b9e6-b6b55a200739' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5f724607-a404-402e-a59a-628665cb4223' class='xr-var-data-in' type='checkbox'><label for='data-5f724607-a404-402e-a59a-628665cb4223' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>ancillary_variables :</span></dt><dd>nobs</dd><dt><span>coverage_content_type :</span></dt><dd>physicalMeasurement</dd><dt><span>height :</span></dt><dd>10 meters above sea-surface</dd><dt><span>long_name :</span></dt><dd>wind speed at 10 meters</dd><dt><span>standard_name :</span></dt><dd>wind_speed</dd><dt><span>units :</span></dt><dd>m s-1</dd><dt><span>valid_max :</span></dt><dd>140.0</dd><dt><span>valid_min :</span></dt><dd>0.0</dd></dl></div><div class='xr-var-data'><pre>ManifestArray&lt;shape=(1456, 720, 1440), dtype=float32, chunks=(1, 720, 1440)&gt;</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>nobs</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>ManifestArray&lt;shape=(1456, 720, ...</div><input id='attrs-b168c7dd-0386-4ec1-bac8-d6663dd9c97c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b168c7dd-0386-4ec1-bac8-d6663dd9c97c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-7ee1f8f8-5eb9-451b-8def-cc575606c0df' class='xr-var-data-in' type='checkbox'><label for='data-7ee1f8f8-5eb9-451b-8def-cc575606c0df' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>coverage_content_type :</span></dt><dd>auxiliaryInformation</dd><dt><span>long_name :</span></dt><dd>number of observations used to derive wind vector components</dd><dt><span>standard_name :</span></dt><dd>number_of_observations</dd><dt><span>valid_max :</span></dt><dd>100.0</dd><dt><span>valid_min :</span></dt><dd>0.0</dd></dl></div><div class='xr-var-data'><pre>ManifestArray&lt;shape=(1456, 720, 1440), dtype=float32, chunks=(1, 720, 1440)&gt;</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>uwnd</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>ManifestArray&lt;shape=(1456, 720, ...</div><input id='attrs-8889c3da-968e-412e-89b6-8e560339ae2c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8889c3da-968e-412e-89b6-8e560339ae2c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-0419f1d6-4a7f-4b95-a1fb-43dd207775b1' class='xr-var-data-in' type='checkbox'><label for='data-0419f1d6-4a7f-4b95-a1fb-43dd207775b1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>ancillary_variables :</span></dt><dd>nobs</dd><dt><span>coverage_content_type :</span></dt><dd>physicalMeasurement</dd><dt><span>height :</span></dt><dd>10 meters above sea-surface</dd><dt><span>long_name :</span></dt><dd>u-wind vector component at 10 meters</dd><dt><span>standard_name :</span></dt><dd>eastward_wind</dd><dt><span>units :</span></dt><dd>m s-1</dd><dt><span>valid_max :</span></dt><dd>100.0</dd><dt><span>valid_min :</span></dt><dd>-100.0</dd></dl></div><div class='xr-var-data'><pre>ManifestArray&lt;shape=(1456, 720, 1440), dtype=float32, chunks=(1, 720, 1440)&gt;</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>vwnd</span></div><div class='xr-var-dims'>(time, latitude, longitude)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>ManifestArray&lt;shape=(1456, 720, ...</div><input id='attrs-9fd82f56-3810-45a7-a0ac-08d47c4146d1' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-9fd82f56-3810-45a7-a0ac-08d47c4146d1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-36a08d15-2c7a-41c7-a479-d38d3972f5e1' class='xr-var-data-in' type='checkbox'><label for='data-36a08d15-2c7a-41c7-a479-d38d3972f5e1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>ancillary_variables :</span></dt><dd>nobs</dd><dt><span>coverage_content_type :</span></dt><dd>physicalMeasurement</dd><dt><span>height :</span></dt><dd>10 meters above sea-surface</dd><dt><span>long_name :</span></dt><dd>v-wind vector component at 10 meters</dd><dt><span>standard_name :</span></dt><dd>northward_wind</dd><dt><span>units :</span></dt><dd>m s-1</dd><dt><span>valid_max :</span></dt><dd>100.0</dd><dt><span>valid_min :</span></dt><dd>-100.0</dd></dl></div><div class='xr-var-data'><pre>ManifestArray&lt;shape=(1456, 720, 1440), dtype=float32, chunks=(1, 720, 1440)&gt;</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-4f445257-5c01-41ea-930e-fc7b274e9908' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-4f445257-5c01-41ea-930e-fc7b274e9908' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-46048066-9dae-4a37-84b0-cbe0c90c6076' class='xr-section-summary-in' type='checkbox'  ><label for='section-46048066-9dae-4a37-84b0-cbe0c90c6076' class='xr-section-summary' >Attributes: <span>(47)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>Conventions :</span></dt><dd>CF-1.7 ACDD-1.3</dd><dt><span>comment :</span></dt><dd>none</dd><dt><span>contact :</span></dt><dd>Remote Sensing Systems, support@remss.com</dd><dt><span>contributor_name :</span></dt><dd>Carl Mears, Tong Lee, Frank Wentz</dd><dt><span>contributor_role :</span></dt><dd>Principal-Investigator, Co-Investigator, Co-Investigator</dd><dt><span>creator_email :</span></dt><dd>support@remss.com</dd><dt><span>creator_name :</span></dt><dd>Remote Sensing Systems</dd><dt><span>creator_type :</span></dt><dd>institution</dd><dt><span>creator_url :</span></dt><dd>http://www.remss.com/</dd><dt><span>data_structure :</span></dt><dd>grid</dd><dt><span>doi :</span></dt><dd>10.5067/CCMP-6HW10M-L4V31</dd><dt><span>geospatial_lat_max :</span></dt><dd>78.375</dd><dt><span>geospatial_lat_min :</span></dt><dd>-78.375</dd><dt><span>geospatial_lat_resolution :</span></dt><dd>0.25 degrees</dd><dt><span>geospatial_lat_units :</span></dt><dd>degrees_north</dd><dt><span>geospatial_lon_max :</span></dt><dd>359.875</dd><dt><span>geospatial_lon_min :</span></dt><dd>0.125</dd><dt><span>geospatial_lon_resolution :</span></dt><dd>0.25 degrees</dd><dt><span>geospatial_lon_units :</span></dt><dd>degrees_east</dd><dt><span>geospatial_vertical_max :</span></dt><dd>10.0</dd><dt><span>geospatial_vertical_min :</span></dt><dd>10.0</dd><dt><span>geospatial_vertical_positive :</span></dt><dd>up</dd><dt><span>geospatial_vertical_units :</span></dt><dd>meters</dd><dt><span>institute_id :</span></dt><dd>RSS</dd><dt><span>institution :</span></dt><dd>Remote Sensing Systems (RSS)</dd><dt><span>instrument_vocabulary :</span></dt><dd>GCMD instrument keywords</dd><dt><span>keywords :</span></dt><dd>EARTH SCIENCE &gt; ATMOSPHERE &gt; ATMOSPHERIC WINDS &gt; SURFACE WINDS, EARTH SCIENCE &gt; OCEANS &gt; OCEAN WINDS, EARTH SCIENCE &gt; ATMOSPHERIC WINDS &gt; SURFACE WINDS &gt; WIND SPEED, EARTH SCIENCE &gt; ATMOSPHERE WINDS &gt; SURFACE WINDS &gt; WIND DIRECTION, 10km - &lt; 50km or approximately 0.09 degree - &lt; 0.5 degree</dd><dt><span>keywords_vocabulary :</span></dt><dd>NASA Global Change Master Directory (GCMD) Science Keywords</dd><dt><span>license :</span></dt><dd>This work is licensed under CC BY 4.0. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/</dd><dt><span>long_name :</span></dt><dd>RSS CCMP 6-Hourly 10 Meter Surface Winds Level 4 Version 3.1</dd><dt><span>naming_authority :</span></dt><dd>gov.nasa</dd><dt><span>platform_vocabulary :</span></dt><dd>GCMD platform keywords</dd><dt><span>processing_level :</span></dt><dd>L4</dd><dt><span>product_version :</span></dt><dd>3.1</dd><dt><span>program :</span></dt><dd>NASA/Ocean Vector Winds Science Team</dd><dt><span>project :</span></dt><dd>RSS Cross-Calibrated Multi-Platform Ocean Surface Wind Project</dd><dt><span>publisher_email :</span></dt><dd>podaac@podaac.jpl.nasa.gov</dd><dt><span>publisher_institution :</span></dt><dd>NASA/JPL/PO.DAAC</dd><dt><span>publisher_name :</span></dt><dd>Physical Oceanography Distributed Active Archive Center, Jet Propulsion Laboratory, NASA</dd><dt><span>publisher_type :</span></dt><dd>institution</dd><dt><span>publisher_url :</span></dt><dd>https://podaac.jpl.nasa.gov</dd><dt><span>references :</span></dt><dd>Mears, C.; Lee, T.; Ricciardulli, L.; Wang, X.; Wentz, F. Improving the Accuracy of the Cross-Calibrated Multi-Platform (CCMP) Ocean Vector Winds. Remote Sens. 2022, 14, 4230. https://doi.org/10.3390/rs14174230, Atlas, R., R. N. Hoffman, J. Ardizzone, S. M. Leidner, J. C. Jusem, D. K. Smith, D. Gombos, 2011: A cross-calibrated, multiplatform ocean surface wind velocity product for meteorological and oceanographic applications. Bull. Amer. Meteor. Soc., 92, 157-174. https://doi.org/10.1175/2010BAMS2946.1</dd><dt><span>short_name :</span></dt><dd>CCMP_WINDS_10M6HR_L4_V3.1</dd><dt><span>source :</span></dt><dd>blend of satellite and ERA-5 winds</dd><dt><span>standard_name_vocabulary :</span></dt><dd>NetCDF Climate and Forecast (CF) Metadata Convention</dd><dt><span>summary :</span></dt><dd>RSS VAM 6-hour analyses using ERA-5 wind reanalyses as background.  Satellite winds (vector and scalar) are assimilated using a variational analysis identical to that used in CCMP V1.1. The ERA5 winds are adjusted to match winds from scatteromters before use in the analysis.</dd><dt><span>title :</span></dt><dd>RSS CCMP V3.1 6-hourly surface winds (Level 4)</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 24GB\n",
       "Dimensions:    (time: 1456, latitude: 720, longitude: 1440)\n",
       "Coordinates:\n",
       "    latitude   (latitude) float32 3kB ManifestArray<shape=(720,), dtype=float...\n",
       "    time       (time) float32 6kB ManifestArray<shape=(1456,), dtype=float32,...\n",
       "    longitude  (longitude) float32 6kB ManifestArray<shape=(1440,), dtype=flo...\n",
       "Data variables:\n",
       "    ws         (time, latitude, longitude) float32 6GB ManifestArray<shape=(1...\n",
       "    nobs       (time, latitude, longitude) float32 6GB ManifestArray<shape=(1...\n",
       "    uwnd       (time, latitude, longitude) float32 6GB ManifestArray<shape=(1...\n",
       "    vwnd       (time, latitude, longitude) float32 6GB ManifestArray<shape=(1...\n",
       "Attributes: (47)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Combine the individual references:\n",
    "vds_combined_year2 = xr.combine_nested(virtual_ds_list_year2, concat_dim='time', coords='minimal', compat='override', combine_attrs='drop_conflicts')\n",
    "vds_combined_year2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb120878-6fd7-42d1-9d75-86fb54ce6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "vds_year1 = open_virtual_dataset('ref_combined_1year.json', filetype='kerchunk')\n",
    "vds_2years = xr.combine_nested([vds_year1, vds_combined_year2], concat_dim='time', coords='minimal', compat='override', combine_attrs='drop_conflicts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39cc7952-bcfc-46c6-b53c-edd4e17f90ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "2916\n"
     ]
    }
   ],
   "source": [
    "## Confirm time dimension has increased in new reference:\n",
    "print(len(vds_year1['time']))\n",
    "print(len(vds_2years['time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1af9234-1aee-4537-ac6c-97e2f2395609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 48GB\n",
      "Dimensions:    (latitude: 720, longitude: 1440, time: 2916)\n",
      "Coordinates:\n",
      "  * latitude   (latitude) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n",
      "  * longitude  (longitude) float32 6kB 0.125 0.375 0.625 ... 359.4 359.6 359.9\n",
      "  * time       (time) datetime64[ns] 23kB 1993-01-02 ... 1995-01-08T18:00:00\n",
      "Data variables:\n",
      "    nobs       (time, latitude, longitude) float32 12GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    uwnd       (time, latitude, longitude) float32 12GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    vwnd       (time, latitude, longitude) float32 12GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "    ws         (time, latitude, longitude) float32 12GB dask.array<chunksize=(1, 720, 1440), meta=np.ndarray>\n",
      "Attributes: (47)\n",
      "CPU times: user 1.72 s, sys: 142 ms, total: 1.86 s\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Save to file then use to open new data set\n",
    "vds_2years.virtualize.to_kerchunk('ref_twoyears.json', format='json')\n",
    "data_2years = opendf_withref('ref_twoyears.json', fs)\n",
    "print(data_2years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc2738-5a47-4356-807a-ae86607a806d",
   "metadata": {},
   "source": [
    "## 4. Using a reference file to analyze data with parallel computing\n",
    "\n",
    "This section verifies that the reference file can be used to perform computations on the CCMP data, additionally verifying that parallel computing can be implemented with the computations. We try parallel computations using both a local and distributed cluster. \n",
    "\n",
    "The analysis will bin/average the first year of CCMP data by month, to generate a \"mean seasonal cycle\" (of course, one year of data isn't enough to produce a real a mean seasonal cycle). The analysis uses Xarray built in functions which naturally parallelize on Dask clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9040928-42a0-44b5-a532-d2ba81169ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_cycle_regional(data_array, lat_region, lon_region):\n",
    "    \"\"\"\n",
    "    Uses built in Xarray functions to generate a mean seasonal cycle at each grid point \n",
    "    over a specified region. Any temporal linear trends are first removed at each point \n",
    "    respecitvely, then data are binned and averaged by month. \n",
    "    \"\"\"\n",
    "    ## Subset to region:\n",
    "    da_regional = data_array.sel(lat=slice(*lat_region), lon=slice(*lon_region))\n",
    "    \n",
    "    ## Remove any linear trends:\n",
    "    p = da_regional.polyfit(dim='time', deg=1) # Degree 1 polynomial fit coefficients over time for each lat, lon.\n",
    "    fit = xr.polyval(da_regional['time'], p.polyfit_coefficients) # Compute linear trend time series at each lat, lon.\n",
    "    da_detrend = (da_regional - fit) # xarray is smart enough to subtract along the time dim only.\n",
    "    \n",
    "    ## Mean seasonal cycle:\n",
    "    seasonal_cycle = da_detrend.groupby(\"time.month\").mean(\"time\")\n",
    "    return seasonal_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "037824cf-1c4e-4122-a457-32e8a7f09e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region to perform analysis over:\n",
    "lat_region = (30, 45)\n",
    "lon_region = (-135, -105)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ea98b-8f78-440d-8ca7-df20ef6cea4c",
   "metadata": {},
   "source": [
    "### 4.1 Using a local cluster\n",
    "This section was run using 16 workers on an `m6i.8xlarge` EC2 instance. Any warning messages generated by the cluster are left in the output here intentionally. Note that despite the warning messages, the parallel computations complete successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3049e68-de11-4171-bce3-524eec799a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count = 32\n"
     ]
    }
   ],
   "source": [
    "print(\"CPU count =\", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f394dec-4fc9-49b0-8015-b9ceb7d62ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local Dask Cluster\n",
    "client = Client(n_workers=16, threads_per_worker=1)\n",
    "print(client.cluster)\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52fd8390-86b6-4dd0-aeb8-f36a2b58cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0491e97-9058-48cc-b02b-b9c95cb7a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.7 s, sys: 7.21 s, total: 38.9 s\n",
      "Wall time: 55.3 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# Perform computations:\n",
    "# seasonal_cycle = seasonal_cycle_regional(<data set>, lat_region, lon_region).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5ee8dfa-b66c-44bf-bf82-66dbce678037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Test plot seasonal cycle at a few gridpoint locations\n",
    "\n",
    "# # Points to plot seasonal cycle at:\n",
    "# lat_points = (38, 38, 38, 38)\n",
    "# lon_points = (-123.25, -125, -128, -132)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = plt.axes()\n",
    "\n",
    "# for lat, lon in zip(lat_points, lon_points):\n",
    "#     scycle_point = seasonal_cycle.sel(lat=lat, lon=lon)\n",
    "#     ax.plot(scycle_point['month'], scycle_point.values, 'o-')\n",
    "\n",
    "# ax.set_title(\"Seasonal cycle of ... anomalies \\n at four test points\", fontsize=14)\n",
    "# ax.set_xlabel(\"month\", fontsize=12)\n",
    "# ax.set_ylabel(r\"$\\Delta$... \", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923baa69-85d3-4718-864f-c67bfc0fc15c",
   "metadata": {},
   "source": [
    "### 4.2 Optional: Using a distributed cluster\n",
    "We use the third party software/package Coiled to spin up our distributed cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6107bc2-f5b9-44f7-8591-5b3c49e7ffe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf54a40c5fb41cfbf330391ca3e84a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b581a0bf1bb4e1c83851a3ccfe9781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=25, \n",
    "    region=\"us-west-2\", \n",
    "    worker_vm_types=\"c7g.large\", # or can try \"m7a.medium\"\n",
    "    scheduler_vm_types=\"c7g.large\", # or can try \"m7a.medium\"\n",
    "    ) \n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf1c67-e9a1-4537-9a73-c78f69dcc307",
   "metadata": {},
   "source": [
    "***Note that computations on the distributed cluster work if we fully load the reference information into memory first, but not if we just pass the path to the reference file!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79eb4578-e229-48e5-920e-705727d61cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 0 ns, total: 3 μs\n",
      "Wall time: 5.01 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##==================================================================\n",
    "## Only works if the reference is loaded into memory first!!!!\n",
    "##==================================================================\n",
    "# Load into memory ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4af68582-641e-4aef-a936-d9b927cbd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# seasonal_cycle = seasonal_cycle_regional(<data set>, lat_region, lon_region).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2219062e-69b1-4bc6-a646-f571c9064e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test plot seasonal cycle at a few gridpoint locations\n",
    "\n",
    "# # Points to plot seasonal cycle at:\n",
    "# lat_points = (38, 38, 38, 38)\n",
    "# lon_points = (-123.25, -125, -128, -132)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = plt.axes()\n",
    "\n",
    "# for lat, lon in zip(lat_points, lon_points):\n",
    "#     scycle_point = seasonal_cycle.sel(lat=lat, lon=lon)\n",
    "#     ax.plot(scycle_point['month'], scycle_point.values, 'o-')\n",
    "\n",
    "# ax.set_title(\"Seasonal cycle of ... anomalies \\n at four test points\", fontsize=14)\n",
    "# ax.set_xlabel(\"month\", fontsize=12)\n",
    "# ax.set_ylabel(r\"$\\Delta$... \", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1dbb25a-f069-4b6f-b607-27edf0fc1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "##==================================================================\n",
    "## Loading the data with the reference this way will lead to errors!!!!\n",
    "##==================================================================\n",
    "# Loading by passing the file path without loading into memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1f937f80-e752-4a52-9cee-b7b695eca9db",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error during deserialization of the task graph. This frequently\noccurs if the Scheduler and Client have different environments.\nFor more information, see\nhttps://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/mapping.py:155\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mcat(k)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_exceptions \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/reference.py:864\u001b[0m, in \u001b[0;36mcat\u001b[0;34m()\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# TODO: if references is lazy, pre-fetch all paths in batch before access\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m proto_dict \u001b[38;5;241m=\u001b[39m _protocol_groups(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreferences)\n\u001b[1;32m    865\u001b[0m out \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/reference.py:50\u001b[0m, in \u001b[0;36m_protocol_groups\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(paths, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {_prot_in_references(paths, references): [paths]}\n\u001b[1;32m     51\u001b[0m out \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/reference.py:43\u001b[0m, in \u001b[0;36m_prot_in_references\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prot_in_references\u001b[39m(path, references):\n\u001b[0;32m---> 43\u001b[0m     ref \u001b[38;5;241m=\u001b[39m references\u001b[38;5;241m.\u001b[39mget(path)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ref, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m<frozen _collections_abc>:807\u001b[0m, in \u001b[0;36mget\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/reference.py:381\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_one_key(key)\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/reference.py:290\u001b[0m, in \u001b[0;36m_load_one_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the reference for one key\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03mReturns bytes, one-element list or three-element list.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items[key]\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/reference.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_items\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecord_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# avoid possible recursion if setup fails somehow\u001b[39;00m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/reference.py:163\u001b[0m, in \u001b[0;36msetup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mcat_file(\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    165\u001b[0m )\n\u001b[1;32m    166\u001b[0m met \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/spec.py:771\u001b[0m, in \u001b[0;36mcat_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# explicitly set buffering off?\u001b[39;00m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/spec.py:1301\u001b[0m, in \u001b[0;36mopen\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1300\u001b[0m ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1301\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[1;32m   1302\u001b[0m     path,\n\u001b[1;32m   1303\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   1304\u001b[0m     block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m   1305\u001b[0m     autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[1;32m   1306\u001b[0m     cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1308\u001b[0m )\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/local.py:195\u001b[0m, in \u001b[0;36m_open\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/local.py:359\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/implementations/local.py:364\u001b[0m, in \u001b[0;36m_open\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/ref_combined_2019.parq/.zmetadata'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/zarr/storage.py:1446\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap[key]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/fsspec/mapping.py:159\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyError\u001b[0m: 'analysed_sst/.zarray'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/zarr/core.py:202\u001b[0m, in \u001b[0;36m_load_metadata_nosync\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m     mkey \u001b[38;5;241m=\u001b[39m _prefix_to_array_key(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_prefix)\n\u001b[0;32m--> 202\u001b[0m     meta_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store[mkey]\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/zarr/storage.py:1448\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'analysed_sst/.zarray'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mArrayNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/distributed/scheduler.py:4686\u001b[0m, in \u001b[0;36mupdate_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4685\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4686\u001b[0m     graph \u001b[38;5;241m=\u001b[39m deserialize(graph_header, graph_frames)\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m   4687\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m graph_header, graph_frames\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/distributed/protocol/serialize.py:449\u001b[0m, in \u001b[0;36mdeserialize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    448\u001b[0m dumps, loads, wants_context \u001b[38;5;241m=\u001b[39m families[name]\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loads(header, frames)\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/distributed/protocol/serialize.py:111\u001b[0m, in \u001b[0;36mpickle_loads\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m buffers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    107\u001b[0m     ensure_writeable_flag(ensure_memoryview(mv), w)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mv, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(buffers, header[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwriteable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    109\u001b[0m ]\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mloads(pik, buffers\u001b[38;5;241m=\u001b[39mbuffers)\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/distributed/protocol/pickle.py:94\u001b[0m, in \u001b[0;36mloads\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffers:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mloads(x, buffers\u001b[38;5;241m=\u001b[39mbuffers)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/zarr/core.py:2572\u001b[0m, in \u001b[0;36m__setstate__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setstate__\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m-> 2572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate)\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/zarr/core.py:170\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# initialize metadata\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_metadata()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# initialize attributes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/zarr/core.py:193\u001b[0m, in \u001b[0;36m_load_metadata\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synchronizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_metadata_nosync()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/zarr/core.py:204\u001b[0m, in \u001b[0;36m_load_metadata_nosync\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArrayNotFoundError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# decode and store metadata as instance members\u001b[39;00m\n",
      "\u001b[0;31mArrayNotFoundError\u001b[0m: array not found at path %r' \"array not found at path %r' 'analysed_sst'\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/xarray/core/dataarray.py:1156\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array. The original is\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mleft unaltered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/xarray/core/dataarray.py:1130\u001b[0m, in \u001b[0;36mDataArray.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1130\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/xarray/core/dataset.py:846\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[0;32m--> 846\u001b[0m evaluated_data \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data):\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/xarray/core/daskmanager.py:70\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[0;34m(self, *data, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: DaskArray, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/distributed/client.py:2234\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2232\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error during deserialization of the task graph. This frequently\noccurs if the Scheduler and Client have different environments.\nFor more information, see\nhttps://docs.dask.org/en/stable/deployment-considerations.html#consistent-software-environments\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# seasonal_cycle = seasonal_cycle_regional(<data set>, lat_region, lon_region).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7937d3d8-a446-4fb4-ac92-13cc01ae8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4a16b-9a8a-421d-8cd1-dbeb984ec8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
