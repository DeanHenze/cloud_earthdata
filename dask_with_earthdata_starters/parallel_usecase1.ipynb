{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "574f488f-29e0-48b2-9c7f-b9855ec19dea",
   "metadata": {},
   "source": [
    "# Parallel Computing with Earthdata and Dask: An Example of Replicating a Function Over Many Files\n",
    "\n",
    "#### *Authors: Dean Henze and Jinbo Wang, NASA JPL PO.DAAC*\n",
    "\n",
    "## Summary\n",
    "\n",
    "A previous notebook covered basic use of Dask for parallel computing with Earthdata. This included the case where we have a function we wish to replicate over many files, represented by the schematic below.\n",
    "\n",
    "<img src=\"./schematic1.png\" alt=\"sch1\" width=\"500\"/>\n",
    "\n",
    "In the previous notebook, a toy example was used to demonstrate this basic functionality using a local cluster and `dask.delayed()`. In this notebook, we expand that workflow to a more complex analysis, representing something closer to a real-world use-case.\n",
    "\n",
    "The analysis will generate global maps of spatial correlation between sea surface temperature (SST) and sea surface height (SSH). The analysis uses PO.DAAC hosted, gridded SSH and SST data sets:\n",
    "* MEaSUREs gridded SSH Version 2205: 0.17° x 0.17° resolution, global map, one file per 5-days, https://doi.org/10.5067/SLREF-CDRV3\n",
    "* GHRSST Level 4 MW_OI Global Foundation SST, V5.0: 0.25° x 0.25° resolution, global map, daily files, https://doi.org/10.5067/GHMWO-4FR05\n",
    "\n",
    "The time period of overlap between these data sets is 1998 – 2020, with 1808 days in total overlapping. For each pair of SST, SSH files on these days, compute a map of spatial correlation between them, where the following method is used at each gridpoint:\n",
    "\n",
    "<img src=\"./schematic_sst-ssh_corr.png\" alt=\"sch_sst-ssh-corr\" width=\"1000\"/>\n",
    "\n",
    "This notebook will first define the functions to read in the data and perform the computations, then test them on a single file. Next a smaller parallel computation will be performed on all pairs of files in 2018 (73 pairs in total), reducing what would have otherwise taken hours to minutes instead. Finally, an optional section will demonstrate what was used to perform the full computation on all 1808 pairs of files at 0.25 degree resolution. \n",
    "\n",
    "## Requirements, prerequisite knowledge, learning outcomes\n",
    "\n",
    "#### Requirements to run this notebook\n",
    "* **Earthdata login account:** An Earthdata Login account is required to access data from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. \n",
    "* **Compute environment:** This notebook can technically be run either in the cloud (AWS instance running in us-west-2), or on a local compute environment (e.g. laptop, server). However, running in the cloud is recommended, since it is unlikely that a laptop has the power to replicate the compute times we quote here. If running in a local environment, the number of workers spun up Section 4 will likely need to be decreased.\n",
    "* **VM type:** For running in AWS (recommended), we used C7i instances, which are \"compute-optimized\" VM's well-suited to the particular computations performed here (more on this later). Since the cost per hour of these instances go up in size, we recommend the following workflow to explore this notebook.\n",
    "> 1. Start by running Sections 1 - 3 in a low cost c7i.2xlarge instance (fractions of a $1 per hour).\n",
    "> 2. For Section 4, run with a c7i.24xlarge. At the time this notebook was written, this VM type took 7-10 minutes to run the computations, and cost ~\\\\$4/hr. You can run a smaller, less expensive VM type, but will need to change one line of code in Section 4.\n",
    "> 3. For Optional Section 5, we ran using a c7i.48xlarge.\n",
    "\n",
    "#### Prerequisite knowledge\n",
    "* Please make sure you are comfortable with the [notebook on Dask basics](https://podaac.github.io/tutorials/notebooks/Advanced_cloud/basic_dask.html) and all prerequisites therein.\n",
    "\n",
    "#### Learning outcomes\n",
    "This notebook demonstrates how to use `dask.delayed()` with a local cluster on an analysis mirroring what someone might want to do in a real-world setting. As such, you will get better insight on how to apply this workflow to your own analysis. Further, this notebook touches briefly on choosing VM types, which may be many user's first introduction to the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff055bc-2f3e-4436-a413-1c6550641f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built in packages\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Math / science packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "# Plotting packages\n",
    "from matplotlib import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Cloud / parallel computing packages\n",
    "import earthaccess\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee13dd-275f-49f0-bc25-3d0fccde6b09",
   "metadata": {},
   "source": [
    "## 1. Define functions\n",
    "\n",
    "The main function implemented is `spatial_corrmap()`, which will return the map of correlations as a 2D array. The other functions below are called by `spatial_corrmap()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4dd16-bfc9-4993-be7d-88f1cd747c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sst_ssh(gran_ssh, gran_sst):\n",
    "    \"\"\"\n",
    "    Return SLA and SST variables for a single file each of the \n",
    "    SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205 and MW_OI-REMSS-L4-GLOB-v5.0 \n",
    "    collections, respectively, returned as xarray.DataArray's. Input args are granule info \n",
    "    (earthaccess.results.DataGranule object's) for each collection.  \n",
    "    \"\"\"\n",
    "    earthaccess.login(strategy=\"environment\") # Confirm credentials are current\n",
    "    \n",
    "    # Get SLA and SST variables, loaded fully into local memory:\n",
    "    ssh = xr.load_dataset(earthaccess.open([gran_ssh], provider='POCLOUD')[0])['SLA'][0,...]\n",
    "    sst = xr.load_dataset(earthaccess.open([gran_sst], provider='POCLOUD')[0])['analysed_sst'][0,...]\n",
    "\n",
    "    return ssh, sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53cc76-82dc-4473-9d61-89be4618ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialcorr(x, y, p1, p2):\n",
    "    \"\"\"\n",
    "    Correlation between two 2D variables p1(x, y), p2(x, y), over the domain (x, y). Correlation is \n",
    "    computed between the anomalies of p1, p2, where anomalies for each variables are the deviations from \n",
    "    respective linear 2D surface fits.\n",
    "    \"\"\"\n",
    "    # Compute anomalies:\n",
    "    ssha, _ = anomalies_2Dsurf(x, y, p1) # See function further down.\n",
    "    ssta, _ = anomalies_2Dsurf(x, y, p2)\n",
    "    \n",
    "    # Compute correlation coefficient:\n",
    "    a, b = ssta.flatten(), ssha.flatten()\n",
    "    if ( np.nansum(abs(a))==0 ) or ( np.nansum(abs(b))==0 ): # There are some cases where all anomalies for one var are 0.\n",
    "        # In this case, correlation should be 0. Numpy will compute this correctly, but will also throw a lot of warnings.\n",
    "        # Get around this by manually appending 0 instead.\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nanmean(a*b)/np.sqrt(np.nanvar(a) * np.nanvar(b))\n",
    "\n",
    "\n",
    "def anomalies_2Dsurf(x, y, p):\n",
    "    \"\"\"\n",
    "    Get anomalies for a variable over a 2D map. Done by fitting a bi-linear 2D surface \n",
    "    to the data (scipy) and taking the anomaly as the difference between each data point \n",
    "    and the surface. Surface can either be a linear or quadratic function.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    x, y: 1D array-like.\n",
    "        Independent vars (likely the lon, lat coordinates).\n",
    "    p: 2D array-like, of shape (len(y), len(x)).\n",
    "        Dependent variable. 2D surface fit will be to p(x, y).\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    va, vm: 2D NumPy arrays\n",
    "        Anomalies (va) and mean surface fit (vm).\n",
    "    \"\"\"\n",
    "    # Functions to (1) output a 2D surface and (2) output the difference between 2D data and the computed surface:\n",
    "    def surface(c,x0,y0): # Takes independent vars and poly coefficients\n",
    "        a,b,c=c\n",
    "        return a + b*x0 + c*y0\n",
    "    \n",
    "    def err(c,x0,y0,p): # Takes independent/dependent vars and poly coefficients\n",
    "        a,b,c=c\n",
    "        return p - (a + b*x0 + c*y0 )\n",
    "\n",
    "\n",
    "    # Prep arrays and remove NAN's:\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    xf=xx.flatten()\n",
    "    yf=yy.flatten()\n",
    "    pf=p.flatten()\n",
    "\n",
    "    msk=~np.isnan(pf)\n",
    "    pf=pf[msk]\n",
    "    xf=xf[msk]\n",
    "    yf=yf[msk]\n",
    "\n",
    "    \n",
    "    # Initial values of polynomial coefficients to start fitting algorithm off with:\n",
    "    dpdx=(pf.max()-pf.min())/(xf.max()-xf.min())\n",
    "    dpdy=(pf.max()-pf.min())/(yf.max()-yf.min())\n",
    "    c = [pf.mean(),dpdx,dpdy]\n",
    "\n",
    "    \n",
    "    # Fit and compute anomalies:\n",
    "    coef = leastsq(err,c,args=(xf,yf,pf))[0]\n",
    "    vm = surface(coef, xx, yy) # mean surface\n",
    "    va = p - vm # anomalies\n",
    "    return va, vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e23d8b-0c6a-4c39-943e-4c37b6cc0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_corrmap(grans, lat_halfwin, lon_halfwin, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Get a 2D map of SSH-SST spatial correlation coefficients, for one each of the \n",
    "    SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205 and MW_OI-REMSS-L4-GLOB-v5.0 collections. \n",
    "    At each gridpoint, the spatial correlation is computed over a lat, lon window of size \n",
    "    2*lat_halfwin x 2*lon_halfwin. Correlation is computed from the SSH, SST anomalies, which are computed \n",
    "    in turn as the deviations from a fitted 2D surface over the window.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    grans: 2-tuple of earthaccess.results.DataGranule objects\n",
    "        Granule info for the SSH, SST files (in that order). These objects contain https and S3 locations.\n",
    "    lat_halfwin, lon_halfwin: floats\n",
    "        Half window size in degrees for latitude and longitude dimensions, respectively.\n",
    "    lats, lons: None or 1D array-like\n",
    "        Latitude, longitude gridpoints at which to compute the correlations. \n",
    "        If None, will use the SSH grid.\n",
    "    f_notnull: float between 0-1 (default = 0.5)\n",
    "        Threshold fraction of non-NAN values in a window in order for the correlation to be computed,\n",
    "        otherwise NAN is returned for that grid point. For edge cases, 'ghost' elements are counted as NAN.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    coef: 2D numpy array\n",
    "        Spatial correlation coefficients.\n",
    "    \n",
    "    lats, lons: 1D numpy arrays.\n",
    "        Latitudes and longitudes creating the 2D grid that 'coef' was calculated on.\n",
    "    \"\"\"    \n",
    "    # Load datafiles, convert SST longitude to (0,360), and interpolate SST to SSH grid:    \n",
    "    ssh, sst = load_sst_ssh(*grans)\n",
    "    sst = sst.roll(lon=len(sst['lon'])//2)\n",
    "    sst['lon'] = sst['lon']+180\n",
    "    sst = sst.interp(lon=ssh['Longitude'], lat=ssh['Latitude'])\n",
    "\n",
    "    \n",
    "    # Compute windows size and threshold number of non-nan points:\n",
    "    dlat = (ssh['Latitude'][1]-ssh['Latitude'][0]).item()\n",
    "    dlon = (ssh['Longitude'][1]-ssh['Longitude'][0]).item()\n",
    "    nx_win = 2*round(lon_halfwin/dlon)\n",
    "    ny_win = 2*round(lat_halfwin/dlat)\n",
    "    n_thresh = nx_win*ny_win*f_notnull\n",
    "\n",
    "\n",
    "    # Some prep work for efficient identification of windows where number of non-nan's < threshold:\n",
    "        # Map of booleans for sst*ssh==np.nan\n",
    "    notnul = (sst*ssh).notnull() \n",
    "        # Combine map and sst, ssh data into single Dataset for more efficient indexing:\n",
    "    notnul = notnul.rename(\"notnul\") # Needs a name to merge\n",
    "    mergeddata = xr.merge([ssh, sst, notnul], compat=\"equals\")\n",
    "     \n",
    "\n",
    "    # Compute spatial correlations over whole map:\n",
    "    coef = []\n",
    "    \n",
    "    if lats is None:\n",
    "        lats = ssh['Latitude'].data\n",
    "        lons = ssh['Longitude'].data\n",
    "    \n",
    "    for lat_cen in lats:\n",
    "        for lon_cen in lons:\n",
    "\n",
    "            # Create window for both sst and ssh with xr.sel:\n",
    "            lat_bottom = lat_cen - lat_halfwin\n",
    "            lat_top = lat_cen + lat_halfwin\n",
    "            lon_left = lon_cen - lon_halfwin\n",
    "            lon_right = lon_cen + lon_halfwin\n",
    "            data_win = mergeddata.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "    \n",
    "            # If number of non-nan values in window is less than threshold \n",
    "            # value, append np.nan, else compute correlation coefficient:\n",
    "            n_notnul = data_win[\"notnul\"].sum().item()\n",
    "            if n_notnul < n_thresh:\n",
    "                coef.append(np.nan)\n",
    "            else:\n",
    "                c = spatialcorr(data_win['Longitude'], data_win['Latitude'], data_win['SLA'].data, data_win['analysed_sst'].data)\n",
    "                coef.append(c)\n",
    "    \n",
    "    return np.array(coef).reshape((len(lats), len(lons))), np.array(lats), np.array(lons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1b430-56d1-412f-8c59-87128a4e45ed",
   "metadata": {},
   "source": [
    "## 2. Get all matching pairs of SSH, SST files for 2018\n",
    "\n",
    "The `spatial_corrmap()` function takes as one of its arguments a 2-tuple of `earthaccess.store.EarthAccessFile` objects, one each for SSH and SST (recall that `earthaccess.store.EarthAccessFile` objects are returned from a call to `earthaccess.open()`, and are passed to `Xarray`). This section will retrieve pairs of these objects for all SSH, SST data in 2018 on days where the data sets overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2069fe9-96cf-41df-97d1-c9e87f10caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64811a-9b86-403e-9dd5-10594219c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Granule info for all files in 2018:\n",
    "dt2018 = (\"2018-01-01\", \"2018-12-31\")\n",
    "grans_ssh = earthaccess.search_data(short_name=\"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\", temporal=dt2018)\n",
    "grans_sst = earthaccess.search_data(short_name=\"MW_OI-REMSS-L4-GLOB-v5.0\", temporal=dt2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc7a3d-e145-4793-afdf-a1197e6cfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## File coverage dates extracted from filenames:\n",
    "dates_ssh = [g['umm']['GranuleUR'].split('_')[-1][:8] for g in grans_ssh]\n",
    "dates_sst = [g['umm']['GranuleUR'][:8] for g in grans_sst]\n",
    "print(' SSH file days: ', dates_ssh[:8], '\\n', 'SST file days: ', dates_sst[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edfcfd-1898-47af-ac6b-c44722f3f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate granule info for dates where there are both SSH and SST files:\n",
    "grans_ssh_analyze = []\n",
    "grans_sst_analyze = []\n",
    "for j in range(len(dates_ssh)):\n",
    "    if dates_ssh[j] in dates_sst:\n",
    "        grans_ssh_analyze.append(grans_ssh[j])\n",
    "        grans_sst_analyze.append(grans_sst[dates_sst.index(dates_ssh[j])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ad58e-3720-4189-9678-c036f12f50ba",
   "metadata": {},
   "source": [
    "**The result is two lists of `earthaccess.results.DataGranule` objects, where the ith element of the SSH, SST lists contain granule info for the respective data sets on the same day:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad200075-c528-4943-bf94-a519ffe0cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grans_ssh_analyze[0]['umm']['CollectionReference']['ShortName'], ':', len(grans_ssh_analyze), 'granules')\n",
    "print([g['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'] for g in grans_ssh_analyze[:4]])\n",
    "print(grans_sst_analyze[0]['umm']['CollectionReference']['ShortName'], ':', len(grans_sst_analyze), 'granules')\n",
    "print([g['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'] for g in grans_sst_analyze[:4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3636ae-d8af-4d64-9bd1-d80146a54917",
   "metadata": {},
   "source": [
    "## 3. Test the computation on a pair of files, output on a coarse resolution grid\n",
    "\n",
    "To verify the functions work, we test them on the first pair of files. To reduce computation time, we compute them for a 2 degree x 2 degree output grid for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03e8f0-bea8-430d-8b1d-fa156661cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spatial correlation map for 2 degree x 2 degree resolution and time it:\n",
    "t1 = time.time()\n",
    "\n",
    "lats = np.arange(-80, 80, 2)\n",
    "lons = np.arange(0, 359, 2)\n",
    "coef, lats, lons = spatial_corrmap((grans_ssh_analyze[0], grans_sst_analyze[0]), 3, 3, lats=lats, lons=lons, f_notnull=0.5)\n",
    "\n",
    "t2 = time.time()\n",
    "comptime = round(t2-t1, 2)\n",
    "print(\"Total computation time = \" + str(comptime) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeedc7b-446f-475c-9e74-3d968ccecbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the results:\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.contourf(lons, lats, coef, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfaabf5-4e9a-47c0-a3cc-2c3cd1e6b449",
   "metadata": {},
   "source": [
    "### Estimation of computation time for higher resolution output and more files\n",
    "\n",
    "The computation for one file computed on a 2 x 2 degree grid takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa50edd-81f7-4965-b3c3-b59aacfe8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(comptime) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493cc0c-e571-489a-b2da-7a959f59ad56",
   "metadata": {},
   "source": [
    "then assuming linear scaling, processing one file at 0.5 x 0.5 degree resolution would take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8e45b-d352-4c16-9647-6796cddc1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_fullres_seconds = comptime*(2/0.5)*(2/0.5)\n",
    "eta_fullres_minutes = round(eta_fullres_seconds/60)\n",
    "print(str(eta_fullres_minutes) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45afc1-86e4-402c-a0a1-2c58d5c82c0c",
   "metadata": {},
   "source": [
    "and for the record over all of 2018 would take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339cce06-7228-45ce-b254-8f2c611b5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_allfiles_hrs = round( (len(grans_ssh)*eta_fullres_minutes)/60, 1 )\n",
    "eta_allfiles_days = round(eta_allfiles_hrs/24, 2)\n",
    "print(str(len(grans_ssh)) + \" granules for 2018.\")\n",
    "print(str(eta_allfiles_hrs) + \" hours = \" + str(eta_allfiles_days) + \" days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28353293-d066-4be4-9748-10471d5cce07",
   "metadata": {},
   "source": [
    "## 4. Parallel computations with Dask\n",
    "\n",
    "The previous section showed that analyzing a year's worth of data at 0.5 x 0.5 degree output resolution would take hours. In this section, we use `dask.delayed()` and a local cluster to complete this task in 7 - 20 minutes, depending on the VM type used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc24ba5-f83e-496e-a0dc-f3afc1831d2e",
   "metadata": {},
   "source": [
    "First, define a wrapper function which calls `spatial_corrmap()` for a pair of SSH, SST files and saves the output to a netCDF file. We will parallelize this function rather than `spatial_corrmap()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e203e7-4abb-4047-9c24-21474dc8751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrmap_tofile(grans, dir_results=\"./\", lat_halfwin=3, lon_halfwin=3, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Calls spatial_corrmap() for a pair of SSH, SST granules and saves the results to a netCDF file.\n",
    "    \"\"\"\n",
    "    coef, lats, lons = spatial_corrmap(grans, lat_halfwin, lon_halfwin, lats=lats, lons=lons, f_notnull=0.5)\n",
    "    date = grans[0]['umm']['GranuleUR'].split(\"_\")[-1][:8] # get date from SSH UR.\n",
    "    corrmap_da = xr.DataArray(data=coef, dims=[\"lat\", \"lon\"], coords=dict(lon=lons, lat=lats), name='corr_ssh_sst')\n",
    "    corrmap_da.to_netcdf(dir_results+\"spatial-corr-map_ssh-sst_\" + date + \".nc\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d64e8-ad02-4df4-87df-4ec7cc7d500d",
   "metadata": {},
   "source": [
    "Next, some prep work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1310c-fc47-4902-889a-ad8788e4f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All output will be saved to this local directory:\n",
    "dir_results = \"results/\"\n",
    "os.makedirs(dir_results, exist_ok=True)\n",
    "\n",
    "# Latitudes, longitudes of output grid at 0.5 degree resolution:\n",
    "lats = np.arange(-80, 80, 0.5)\n",
    "lons = np.arange(0, 359, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35bdb1-15e6-4523-8e90-b584564db7fb",
   "metadata": {},
   "source": [
    "Start up the cluster. We used a c7i.24xlarge EC2 instance type, which has 96 vCPU's, and therefore we are able to start up 73 workers. If you use a smaller instance type in the C7i series, change the `n_workers` arg as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf462967-48d4-4298-883f-e653ae465822",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=73, threads_per_worker=1)\n",
    "print(client.cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa3c5f-8977-4deb-9da3-d60330c5466d",
   "metadata": {},
   "source": [
    "This block of code will give each worker our EDL credentials (even though we logged in with `earthaccess()` this is necessary to get creds to each worker):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6acf9-6660-4659-bcba-b3781279cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_env(auth): # this gets executed on each worker\n",
    "    os.environ[\"EARTHDATA_USERNAME\"] = auth[\"EARTHDATA_USERNAME\"]\n",
    "    os.environ[\"EARTHDATA_PASSWORD\"] = auth[\"EARTHDATA_PASSWORD\"]\n",
    "    \n",
    "_ = client.run(auth_env, auth=earthaccess.auth_environ())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305a752-3699-4f7f-9e31-d7f1b212e5ca",
   "metadata": {},
   "source": [
    "then setup the parallel computations and run!\n",
    "\n",
    "(At the time this notebook was written, `earthaccess` produces a lot of output each time a file is opened, and so the output from this cell is long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48eb3ce-d024-4ef4-8f98-b9c5607b232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# Process granules in parallel using Dask:\n",
    "grans_2tuples = list(zip(grans_ssh_analyze, grans_sst_analyze))\n",
    "corrmap_tofile_parallel = delayed(corrmap_tofile)\n",
    "tasks = [\n",
    "    corrmap_tofile_parallel(g2, dir_results=\"results/\", lats=lats, lons=lons, f_notnull=0.5) \n",
    "    for g2 in grans_2tuples[:]\n",
    "    ]                     # Sets up the computations (relatively quick)\n",
    "_ = da.compute(*tasks)    # Performs the computations (takes most of the time)\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29771b-12f4-48aa-9a4b-10631d174955",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What was the total computation time?\n",
    "comptime = round(t2-t1, 2)\n",
    "print(\"Total computation time = \" + str(comptime) + \" seconds = \" + str(comptime/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb107e4-a2c9-4968-884a-ac16ba23aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"distributed.admin.system-monitor.gil.enabled\": False}) # Adjust settings to suppress noisy output when closing cluster\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a020bf1-8aee-4d29-8858-217f52d94ed6",
   "metadata": {},
   "source": [
    "### Test plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e76b8-5bf5-4ebb-9966-2557d24fa450",
   "metadata": {},
   "source": [
    "**Colormaps of first three files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecf856-86da-45fd-b0a9-4835a5695268",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_results = [dir_results + f for f in os.listdir(dir_results) if f.endswith(\"nc\")]\n",
    "for fn in fns_results[:3]:\n",
    "    testfile = xr.open_dataset(fn)\n",
    "    testfile[\"corr_ssh_sst\"].plot(figsize=(6,2), vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "    testfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b78194-f8df-4ab1-bdab-398c3d45446c",
   "metadata": {},
   "source": [
    "**Colormap of mean correlation map for all files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bea98-7730-4f6a-a23d-8f65b6aeeb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_allfiles = xr.open_mfdataset(fns_results, combine='nested', concat_dim='dummy_time')\n",
    "test_allfiles[\"corr_ssh_sst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd40ea-b554-4e87-b918-380e283ef0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_allfiles[\"corr_ssh_sst\"].mean(dim='dummy_time').plot(figsize=(8,3), vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "test_allfiles.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52d331-91da-46d6-a0c2-5b428e491a80",
   "metadata": {},
   "source": [
    "## Other notes\n",
    "\n",
    "* **Why C7i instance types?** Notice that the size of our data files are relatively small, about 10 MB per pair of files. Yet, the computations we perform on them are complex enough that it takes ~5-7 minutes per pair of files (and this is only at 0.5 degree output resolution, multiply this by 4x for the 0.25 degree resolution in Section 5!). This type of computation is referred to as \"compute limited\", because the limitting factor in the time for completion is churning through the computation itself. Contrast this to e.g. a \"memory limited\" computation, where perhaps the computation is simple but the size of each file is large (an example would be taking the global average of a MUR 0.01 degree file). As per [Amazon's page describing the different classes of EC2 types](https://aws.amazon.com/ec2/instance-types/), the C7i series are compute optimized, and therefore well suited to this problem. For a given amount of total memory in the VM, we get a lot of high performance processors, and each one can churn through the computations per pair of SST-SSH files. *Please take this explanation with a grain of salt. The author is not a computer scientist and is learning these complex topics himself!*\n",
    "* **Scaling up to the full computation:** In the section below, we process the full time record at 0.25 degree resolution. We tried this with both the c7i.24xlarge and c7i.48xlarge EC2 instances, which differ only in number of processors (the latter has 2x the former). Unsurprisignly, the latter takes about half the time of the former, but ulti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49821d18-ea46-4632-8e87-b05df5486db1",
   "metadata": {},
   "source": [
    "## 5. Optional: Parallel computation on full record at 0.25 degree resolution\n",
    "\n",
    "Only Section 1 needs to be run prior to this. Sections 2-4 can be skipped.\n",
    "\n",
    "This section mirrors the workflow of Section 4, but processes all 1808 pairs of files, spanning a little over two decades, at higher resolution. To get an estimate of how long this would take without parallel computing, you can re-run section 3 but replace a value of *0.5* for the `higher_res` variable with *0.25* (in the part where we estimate comp times). Trying this on a few machines, we get that it would take anywhere from 21 to 34 hours to process the record over a single year, which means for 22 years it would take 19 to 31 days to complete the entire record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c5236-4aff-414a-b693-9e925f4b5c2d",
   "metadata": {},
   "source": [
    "First, we duplicate most of the code in Section 2, this time getting granule info objects for the entire record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48c196-605c-491d-91c4-04a0564ed9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.login()\n",
    "\n",
    "## Granule info for all files in both collections:\n",
    "grans_ssh = earthaccess.search_data(short_name=\"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\")\n",
    "grans_sst = earthaccess.search_data(short_name=\"MW_OI-REMSS-L4-GLOB-v5.0\")\n",
    "\n",
    "## File coverage dates extracted from filenames:\n",
    "dates_ssh = [g['umm']['GranuleUR'].split('_')[-1][:8] for g in grans_ssh]\n",
    "dates_sst = [g['umm']['GranuleUR'][:8] for g in grans_sst]\n",
    "\n",
    "## Separate granule info for dates where there are both SSH and SST files:\n",
    "grans_ssh_analyze = []\n",
    "grans_sst_analyze = []\n",
    "for j in range(len(dates_ssh)):\n",
    "    if dates_ssh[j] in dates_sst:\n",
    "        grans_ssh_analyze.append(grans_ssh[j])\n",
    "        grans_sst_analyze.append(grans_sst[dates_sst.index(dates_ssh[j])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c9096-5837-4ae5-9975-69773675f09f",
   "metadata": {},
   "source": [
    "Same wrapper function as in section 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f809ad-e15c-4e7a-8195-2a09d8d7aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrmap_tofile(grans, dir_results=\"./\", lat_halfwin=3, lon_halfwin=3, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Calls spatial_corrmap() for a pair of SSH, SST granules and saves the results to a netCDF file.\n",
    "    \"\"\"\n",
    "    coef, lats, lons = spatial_corrmap(grans, lat_halfwin, lon_halfwin, lats=lats, lons=lons, f_notnull=0.5)\n",
    "    date = grans[0]['umm']['GranuleUR'].split(\"_\")[-1][:8] # get date from SSH UR.\n",
    "    corrmap_da = xr.DataArray(data=coef, dims=[\"lat\", \"lon\"], coords=dict(lon=lons, lat=lats), name='corr_ssh_sst')\n",
    "    corrmap_da.to_netcdf(dir_results+\"spatial-corr-map_ssh-sst_\" + date + \".nc\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecd9d4-55ae-48ff-af08-5dc0ff38f63a",
   "metadata": {},
   "source": [
    "Some prep work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e009f-2fad-4362-bea7-a58ab1d4c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the local directory to store results in:\n",
    "dir_results = \"results/\"\n",
    "if os.path.exists(dir_results) and os.path.isdir(dir_results):\n",
    "    shutil.rmtree(dir_results)\n",
    "os.makedirs(dir_results)\n",
    "\n",
    "# Latitudes, longitudes of output grid at 0.5 degree resolution:\n",
    "lats = np.arange(-80, 80.1, 0.25)\n",
    "lons = np.arange(0, 360, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23c27a-ec23-426f-aecf-e2c9bd0d8f17",
   "metadata": {},
   "source": [
    "#### ***!!! Big Cluster !!!***\n",
    "\n",
    "**We used a c7i.48xlarge EC2 instance, with a total of 192 vCPUs and 384 GiB of memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105c698-8c0e-421a-906b-2523b6fb3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = Client(n_workers=96, threads_per_worker=1) # Use this line if working on a c7i.24xlarge EC2 instance\n",
    "client = Client(n_workers=170, threads_per_worker=1) # Use this line if working on a c7i.48xlarge EC2 instance\n",
    "print(client.cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd5d6d-f5fe-422b-b7f3-09e65ddf1afe",
   "metadata": {},
   "source": [
    "give each worker our EDL credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76081630-a89b-4839-9ed9-62e71112395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_env(auth): # this gets executed on each worker\n",
    "    os.environ[\"EARTHDATA_USERNAME\"] = auth[\"EARTHDATA_USERNAME\"]\n",
    "    os.environ[\"EARTHDATA_PASSWORD\"] = auth[\"EARTHDATA_PASSWORD\"]\n",
    "    \n",
    "_ = client.run(auth_env, auth=earthaccess.auth_environ())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1c6b5-cd74-4de8-8c2a-aeda384922d6",
   "metadata": {},
   "source": [
    "then setup the parallel computations and run!\n",
    "\n",
    "(At the time this notebook was written, `earthaccess` produces a lot of output each time a file is opened, and so the output from this cell is long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f128f-7eed-4e2d-a8cb-d28157a38452",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "# Process granules in parallel using Dask:\n",
    "grans_2tuples = list(zip(grans_ssh_analyze, grans_sst_analyze))\n",
    "corrmap_tofile_parallel = delayed(corrmap_tofile)\n",
    "tasks = [\n",
    "    corrmap_tofile_parallel(g2, dir_results=\"results/\", lats=lats, lons=lons, f_notnull=0.5) \n",
    "    for g2 in grans_2tuples[:]\n",
    "    ]                     # Sets up the computations (relatively quick)\n",
    "_ = da.compute(*tasks)    # Performs the computations (takes most of the time)\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204613c-2bdc-4ec1-b6e3-03a5af9d9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"distributed.admin.system-monitor.gil.enabled\": False}) # Adjust settings to suppress noisy output when closing cluster\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5265151-5e68-4f0e-957c-267d36cd873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What was the total computation time?\n",
    "comptime = round(t2-t1, 2)\n",
    "print(\"Total computation time = \" + str(comptime) + \" seconds = \" + str(comptime/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c0139-e229-449b-9037-72849a74efdb",
   "metadata": {},
   "source": [
    "### Test plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6fc24-1825-4e9b-aefa-58ef9d678e95",
   "metadata": {},
   "source": [
    "**Colormaps of first three files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360f65b-4643-498a-bfd7-6a2925a3de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_results = [dir_results + f for f in os.listdir(dir_results) if f.endswith(\"nc\")]\n",
    "for fn in fns_results[:3]:\n",
    "    testfile = xr.open_dataset(fn)\n",
    "    testfile[\"corr_ssh_sst\"].plot(figsize=(6,2), vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "    testfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b590ee-fa9d-48cf-beb7-895a3957602f",
   "metadata": {},
   "source": [
    "**Colormap of mean correlation map for all files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981915bf-903d-40da-8ed9-8a8e039147bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_allfiles = xr.open_mfdataset(fns_results, combine='nested', concat_dim='dummy_time')\n",
    "test_allfiles[\"corr_ssh_sst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2629c9-d2a7-4b09-84d5-a7308d6d2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_allfiles[\"corr_ssh_sst\"].mean(dim='dummy_time').plot(figsize=(8,3), vmin=-1, vmax=1, cmap='RdBu_r')\n",
    "test_allfiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3d3fd-b3d0-48d8-b6c3-e669a43192a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
