{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e503f448-5434-4db6-a9a7-bdc883abc82a",
   "metadata": {},
   "source": [
    "## `spatial_corrmap` function and supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559547cc-948c-4f8c-9b51-5ee6c0e03228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from matplotlib import pylab as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numpy import zeros,arange\n",
    "from scipy.optimize import leastsq\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "import earthaccess\n",
    "\n",
    "\n",
    "def spatial_corrmap(granule_ssh, lat_halfwin, lon_halfwin, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Get a 2D map of SSH-SST spatial correlation coefficients. The SSH dataset is \n",
    "    shortname SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812 \n",
    "    and the SST dataset is shortname MW_OI-REMSS-L4-GLOB-v5.0. \n",
    "\n",
    "    At each gridpoint, the spatial correlation is computed over a lat, lon window\n",
    "    of size 2*lat_halfwin x 2*lon_halfwin. This is done for each gridpoint in\n",
    "    the datasets. Spatial correlation is computed from the SSH, SST anomalies, \n",
    "    which are computed in turn as the deviations from a fitted 2D surface over \n",
    "    the window (a new 2D surface is fitted for each window around each gridpoint).\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    granule_ssh: earthaccess.results.DataGranule\n",
    "        Granule info for theSSH file, including download path and S3 location.  \n",
    "    lat_halfwin, lon_halfwin: floats\n",
    "        Half window size in degrees for latitude and longitude dimensions, respectively.\n",
    "    lats, lons: None or 1D array-like\n",
    "        These make up the latitude, longitude grid on which to compute correlations. \n",
    "        If None, will default to using the grid of the SSH product. Note that regardless\n",
    "        of the lats, lons passed to this function, it will still use the gridpoints \n",
    "        of the SSH product for the actual computations.\n",
    "    f_notnull: float (default = 50)\n",
    "        Fraction of elements in a window that have to be non-nan. Percentage is computed\n",
    "        as number of null elements divided by total number expected to be in the window. So\n",
    "        for edge cases, 'ghost' elements at the edges are considered nan.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    coef: 2D numpy array\n",
    "        Spatial correlation coefficients.\n",
    "    \n",
    "    lats, lons: 1D numpy arrays.\n",
    "        Latitudes and longitudes creating the 2D grid that 'coef' was calculated on.\n",
    "    \"\"\"    \n",
    "    # Load datafiles, convert SST lon to (0,360) bounds, and interpolate SST to SSH grid:    \n",
    "    ssh,sst = load_sst_ssh(granule_ssh)\n",
    "    sst = sst.roll(lon=len(sst['lon'])//2)\n",
    "    sst['lon'] = sst['lon']+180\n",
    "    sst = sst.interp(lon=ssh['Longitude']).interp(lat=ssh['Latitude'])\n",
    "\n",
    "    \n",
    "    # Compute windows size and threshold number of non-nan points:\n",
    "    dlat = (ssh['Latitude'][1]-ssh['Latitude'][0]).item()\n",
    "    dlon = (ssh['Longitude'][1]-ssh['Longitude'][0]).item()\n",
    "    nx_win = 2*round(lon_halfwin/dlon)\n",
    "    ny_win = 2*round(lat_halfwin/dlat)\n",
    "    n_thresh = nx_win*ny_win*f_notnull\n",
    "\n",
    "\n",
    "    # Map of booleans for sst*ssh==np.nan. Will be used to determine if there are \n",
    "    # enough non-nan values to compute the correlation for a given window:\n",
    "    notnul = (sst*ssh).notnull()\n",
    "\n",
    "    \n",
    "    # Compute spatial correlations over whole map:\n",
    "    coef = []\n",
    "    \n",
    "    if lats is None:\n",
    "        lats = ssh['Latitude'].data\n",
    "        lons = ssh['Longitude'].data\n",
    "    \n",
    "    for lat_cen in lats:\n",
    "        for lon_cen in lons:\n",
    "\n",
    "            # Create window for both sst and ssh with xr.sel:\n",
    "            lat_bottom = lat_cen - lat_halfwin\n",
    "            lat_top = lat_cen + lat_halfwin\n",
    "            lon_left = lon_cen - lon_halfwin\n",
    "            lon_right = lon_cen + lon_halfwin\n",
    "            ssh_win = ssh.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "            sst_win = sst.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "    \n",
    "            # If number of non-nan values in sst*ssh window is less than threshold \n",
    "            # value, append np.nan, else compute anomalies and append their correlation coefficient:\n",
    "            notnul_win = notnul.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "            n_notnul = notnul_win.sum().item()\n",
    "            if n_notnul < n_thresh:\n",
    "                coef.append(np.nan)\n",
    "            else:\n",
    "                # Compute anomalies:\n",
    "                ssha,_=anomaly(ssh_win['Longitude'], ssh_win['Latitude'], ssh_win.data)\n",
    "                ssta,_=anomaly(sst_win['Longitude'], sst_win['Latitude'], sst_win.data)\n",
    "                \n",
    "                # Compute correlation coefficient:\n",
    "                a, b = ssta.flatten(), ssha.flatten()\n",
    "                if ( np.nansum(abs(a))==0 ) or ( np.nansum(abs(b))==0 ): # There are some cases where all anomalies for one var are 0.\n",
    "                    coef.append(0) # In this case, correlation should be 0. Numpy will compute this correctly, but will also throw a lot of warnings.\n",
    "                else:\n",
    "                    c = np.nanmean(a*b)/np.sqrt(np.nanvar(a) * np.nanvar(b))\n",
    "                    coef.append(c)\n",
    "        \n",
    "            \n",
    "    return np.array(coef).reshape((len(lats), len(lons))), np.array(lats), np.array(lons)\n",
    "\n",
    "\n",
    "def load_sst_ssh(granule_ssh):\n",
    "    \"\"\"\n",
    "    Return data for a single file each of SSH and SST on the same day. \n",
    "    Input arg is SSH granule info (earthaccess.results.DataGranule object) \n",
    "    for a file from the SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812 \n",
    "    collection. Output is SLA data from the SSH granule along with SST data from the \n",
    "    MW_OI-REMSS-L4-GLOB-v5.0 collection, at timestamp noon UTC of the same day. \n",
    "    Returns ssh, sst as xarray.DataArray's.    \n",
    "    \"\"\"  \n",
    "    # Get SLA variable from file, loaded fully into local memory:\n",
    "    ssh = xr.load_dataset(earthaccess.open([granule_ssh])[0])['SLA'][0,...]\n",
    "    \n",
    "    # Get SST variable from SST file on same day as ssh_fn and at noon. Again, fully loaded into local memory:\n",
    "    date = granule_ssh['umm']['GranuleUR'].split('_')[-1][:8]\n",
    "    s3path_sst = 's3://podaac-ops-cumulus-protected/MW_OI-REMSS-L4-GLOB-v5.0/%s120000-REMSS-L4_GHRSST-SSTfnd-MW_OI-GLOB-v02.0-fv05.0.nc'%date\n",
    "    sst = xr.load_dataset(earthaccess.open([s3path_sst], provider='POCLOUD')[0])['analysed_sst'][0,...]\n",
    "    \n",
    "    return ssh, sst\n",
    "\n",
    "\n",
    "def anomaly(lon, lat, p):\n",
    "    \"\"\"\n",
    "    Get anomalies for a variable over a 2D map. Done by fitting a 2D surface \n",
    "    to the data and taking the anomaly as the difference between each data point \n",
    "    and the surface. \n",
    "    \n",
    "    This is mostly a wrapper for fit2Dsurf() which does the anomaly calculation. \n",
    "    This wrapper could be extended e.g. to take inputs with various shapes and \n",
    "    reformat them to work with fit2Dsurf(), but curretly has basic functionality.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    lon, lat: 1D array-like\n",
    "        Longitude and latitude arrays, or more generally, the x, y coordinates (don't \n",
    "        need to have units of degrees e.g.).\n",
    "    p: 2D array-like\n",
    "        Variable to get anomalies for. Should have same shape as (lat, lon). \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    va, vm: 2D NumPy arrays\n",
    "        Anomalies (va) and mean surface fit (vm).\n",
    "\n",
    "    Import requirements\n",
    "    -------------------\n",
    "    numpy    \n",
    "    \"\"\"\n",
    "    x1, y1 = np.meshgrid(lon, lat)\n",
    "    va, vm = fit2Dsurf(x1, y1, p)\n",
    "    return va,vm\n",
    "\n",
    "\n",
    "def fit2Dsurf(x, y, p, kind='linear'):\n",
    "    \"\"\"\n",
    "    Get anomalies for a variable over a 2D map. Done by fitting a 2D surface \n",
    "    to the data and taking the anomaly as the difference between each data point \n",
    "    and the surface. Surface can either be a linear or quadratic function.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    x, y, p: 2D array-like, all same size.\n",
    "        Variables to use to fit the function p(x, y). x, y are the dependent vars\n",
    "        and p is the dependent var.\n",
    "    kind: str\n",
    "        (Default = 'linear'). Either 'linear' or 'quadratic' to specify the \n",
    "        functional form of the fit surface.\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    va, vm: 2D NumPy arrays\n",
    "        Anomalies (va) and mean surface fit (vm).\n",
    "\n",
    "    Import requirements\n",
    "    -------------------\n",
    "    from scipy.optimize import leastsq\n",
    "    numpy\n",
    "    \"\"\"\n",
    "    # Depending on fit fxn chosen, define functions to output a 2D surface (surface()) \n",
    "    # and the difference between 2D data and the computed surface (err()). Each \n",
    "    # fxn takes independent vars and polynomial coefficients; the err() fxn's in \n",
    "    # addition take data for the dependent var.\n",
    "    if kind=='linear':\n",
    "        def err(c,x0,y0,p):\n",
    "            a,b,c=c\n",
    "            return p - (a + b*x0 + c*y0 )\n",
    "\n",
    "        def surface(c,x0,y0):\n",
    "            a,b,c=c\n",
    "            return a + b*x0 + c*y0\n",
    "\n",
    "    if kind=='quadratic':\n",
    "        def err(c,x0,y0,p):\n",
    "            a,b,c,d,e,f=c\n",
    "            return p - (a + b*x0 + c*y0 + d*x0**2 + e*y0**2 + f*x0*y0)\n",
    "        \n",
    "        def surface(c,x0,y0):\n",
    "            a,b,c,d,e,f=c\n",
    "            return a + b*x0 + c*y0 + d*x0**2 + e*y0**2 + f*x0*y0\n",
    "\n",
    "\n",
    "    # Prep arrays and remove NAN's:\n",
    "    xf=x.flatten()\n",
    "    yf=y.flatten()\n",
    "    pf=p.flatten()\n",
    "\n",
    "    msk=~np.isnan(pf)\n",
    "    pf=pf[msk]\n",
    "    xf=xf[msk]\n",
    "    yf=yf[msk]\n",
    "\n",
    "    \n",
    "    # Initial values of polynomial coefficients to start fitting algorithm off with:\n",
    "    dpdx=(pf.max()-pf.min())/(xf.max()-xf.min())\n",
    "    dpdy=(pf.max()-pf.min())/(yf.max()-yf.min())\n",
    "    if kind=='linear':\n",
    "        c = [pf.mean(),dpdx,dpdy]\n",
    "    if kind=='quadratic':\n",
    "        c = [pf.mean(),dpdx,dpdy,1e-22,1e-22,1e-22]\n",
    "\n",
    "\n",
    "    # Fit:\n",
    "    coef = leastsq(err,c,args=(xf,yf,pf))[0]\n",
    "    vm = surface(coef,x,y) #mean surface\n",
    "    va = p - vm #anomaly\n",
    "    return va,vm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab052015-4af0-4381-9730-39d473b17641",
   "metadata": {},
   "source": [
    "### Copy the following block of code to a .py file *\"ssh_sst_correlation_test.py\"*, with the `@profile` decorator on the `spatial_corrmap()` function\n",
    "The code includes all the functions along with a single call to the `spatial_corrmap` function with specified args."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f93d38-c3c3-4fa3-a96b-a64c71d5485a",
   "metadata": {},
   "source": [
    "import time\n",
    "from matplotlib import pylab as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numpy import zeros,arange\n",
    "from scipy.optimize import leastsq\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "import earthaccess\n",
    "\n",
    "\n",
    "@profile\n",
    "def spatial_corrmap(granule_ssh, lat_halfwin, lon_halfwin, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Get a 2D map of SSH-SST spatial correlation coefficients. The SSH dataset is \n",
    "    shortname SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812 \n",
    "    and the SST dataset is shortname MW_OI-REMSS-L4-GLOB-v5.0. \n",
    "\n",
    "    At each gridpoint, the spatial correlation is computed over a lat, lon window\n",
    "    of size 2*lat_halfwin x 2*lon_halfwin. This is done for each gridpoint in\n",
    "    the datasets. Spatial correlation is computed from the SSH, SST anomalies, \n",
    "    which are computed in turn as the deviations from a fitted 2D surface over \n",
    "    the window (a new 2D surface is fitted for each window around each gridpoint).\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    granule_ssh: earthaccess.results.DataGranule\n",
    "        Granule info for theSSH file, including download path and S3 location.  \n",
    "    lat_halfwin, lon_halfwin: floats\n",
    "        Half window size in degrees for latitude and longitude dimensions, respectively.\n",
    "    lats, lons: None or 1D array-like\n",
    "        These make up the latitude, longitude grid on which to compute correlations. \n",
    "        If None, will default to using the grid of the SSH product. Note that regardless\n",
    "        of the lats, lons passed to this function, it will still use the gridpoints \n",
    "        of the SSH product for the actual computations.\n",
    "    f_notnull: float (default = 50)\n",
    "        Fraction of elements in a window that have to be non-nan. Percentage is computed\n",
    "        as number of null elements divided by total number expected to be in the window. So\n",
    "        for edge cases, 'ghost' elements at the edges are considered nan.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    coef: 2D numpy array\n",
    "        Spatial correlation coefficients.\n",
    "    \n",
    "    lats, lons: 1D numpy arrays.\n",
    "        Latitudes and longitudes creating the 2D grid that 'coef' was calculated on.\n",
    "    \"\"\"    \n",
    "    # Load datafiles, convert SST lon to (0,360) bounds, and interpolate SST to SSH grid:    \n",
    "    ssh,sst = load_sst_ssh(granule_ssh)\n",
    "    sst = sst.roll(lon=len(sst['lon'])//2)\n",
    "    sst['lon'] = sst['lon']+180\n",
    "    sst = sst.interp(lon=ssh['Longitude']).interp(lat=ssh['Latitude'])\n",
    "\n",
    "    \n",
    "    # Compute windows size and threshold number of non-nan points:\n",
    "    dlat = (ssh['Latitude'][1]-ssh['Latitude'][0]).item()\n",
    "    dlon = (ssh['Longitude'][1]-ssh['Longitude'][0]).item()\n",
    "    nx_win = 2*round(lon_halfwin/dlon)\n",
    "    ny_win = 2*round(lat_halfwin/dlat)\n",
    "    n_thresh = nx_win*ny_win*f_notnull\n",
    "\n",
    "\n",
    "    # Map of booleans for sst*ssh==np.nan. Will be used to determine if there are \n",
    "    # enough non-nan values to compute the correlation for a given window:\n",
    "    notnul = (sst*ssh).notnull()\n",
    "\n",
    "    \n",
    "    # Compute spatial correlations over whole map:\n",
    "    coef = []\n",
    "    \n",
    "    if lats is None:\n",
    "        lats = ssh['Latitude'].data\n",
    "        lons = ssh['Longitude'].data\n",
    "    \n",
    "    for lat_cen in lats:\n",
    "        for lon_cen in lons:\n",
    "\n",
    "            # Create window for both sst and ssh with xr.sel:\n",
    "            lat_bottom = lat_cen - lat_halfwin\n",
    "            lat_top = lat_cen + lat_halfwin\n",
    "            lon_left = lon_cen - lon_halfwin\n",
    "            lon_right = lon_cen + lon_halfwin\n",
    "            ssh_win = ssh.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "            sst_win = sst.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "    \n",
    "            # If number of non-nan values in sst*ssh window is less than threshold \n",
    "            # value, append np.nan, else compute anomalies and append their correlation coefficient:\n",
    "            notnul_win = notnul.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "            n_notnul = notnul_win.sum().item()\n",
    "            if n_notnul < n_thresh:\n",
    "                coef.append(np.nan)\n",
    "            else:\n",
    "                # Compute anomalies:\n",
    "                ssha,_=anomaly(ssh_win['Longitude'], ssh_win['Latitude'], ssh_win.data)\n",
    "                ssta,_=anomaly(sst_win['Longitude'], sst_win['Latitude'], sst_win.data)\n",
    "                \n",
    "                # Compute correlation coefficient:\n",
    "                a, b = ssta.flatten(), ssha.flatten()\n",
    "                if ( np.nansum(abs(a))==0 ) or ( np.nansum(abs(b))==0 ): # There are some cases where all anomalies for one var are 0.\n",
    "                    coef.append(0) # In this case, correlation should be 0. Numpy will compute this correctly, but will also throw a lot of warnings.\n",
    "                else:\n",
    "                    c = np.nanmean(a*b)/np.sqrt(np.nanvar(a) * np.nanvar(b))\n",
    "                    coef.append(c)\n",
    "        \n",
    "            \n",
    "    return np.array(coef).reshape((len(lats), len(lons))), np.array(lats), np.array(lons)\n",
    "\n",
    "\n",
    "def load_sst_ssh(granule_ssh):\n",
    "    \"\"\"\n",
    "    Return data for a single file each of SSH and SST on the same day. \n",
    "    Input arg is SSH granule info (earthaccess.results.DataGranule object) \n",
    "    for a file from the SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812 \n",
    "    collection. Output is SLA data from the SSH granule along with SST data from the \n",
    "    MW_OI-REMSS-L4-GLOB-v5.0 collection, at timestamp noon UTC of the same day. \n",
    "    Returns ssh, sst as xarray.DataArray's.    \n",
    "    \"\"\"  \n",
    "    # Get SLA variable from file, loaded fully into local memory:\n",
    "    ssh = xr.load_dataset(earthaccess.open([granule_ssh])[0])['SLA'][0,...]\n",
    "    \n",
    "    # Get SST variable from SST file on same day as ssh_fn and at noon. Again, fully loaded into local memory:\n",
    "    date = granule_ssh['umm']['GranuleUR'].split('_')[-1][:8]\n",
    "    s3path_sst = 's3://podaac-ops-cumulus-protected/MW_OI-REMSS-L4-GLOB-v5.0/%s120000-REMSS-L4_GHRSST-SSTfnd-MW_OI-GLOB-v02.0-fv05.0.nc'%date\n",
    "    sst = xr.load_dataset(earthaccess.open([s3path_sst], provider='POCLOUD')[0])['analysed_sst'][0,...]\n",
    "    \n",
    "    return ssh, sst\n",
    "\n",
    "\n",
    "def anomaly(lon, lat, p):\n",
    "    \"\"\"\n",
    "    Get anomalies for a variable over a 2D map. Done by fitting a 2D surface \n",
    "    to the data and taking the anomaly as the difference between each data point \n",
    "    and the surface. \n",
    "    \n",
    "    This is mostly a wrapper for fit2Dsurf() which does the anomaly calculation. \n",
    "    This wrapper could be extended e.g. to take inputs with various shapes and \n",
    "    reformat them to work with fit2Dsurf(), but curretly has basic functionality.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    lon, lat: 1D array-like\n",
    "        Longitude and latitude arrays, or more generally, the x, y coordinates (don't \n",
    "        need to have units of degrees e.g.).\n",
    "    p: 2D array-like\n",
    "        Variable to get anomalies for. Should have same shape as (lat, lon). \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    va, vm: 2D NumPy arrays\n",
    "        Anomalies (va) and mean surface fit (vm).\n",
    "\n",
    "    Import requirements\n",
    "    -------------------\n",
    "    numpy    \n",
    "    \"\"\"\n",
    "    x1, y1 = np.meshgrid(lon, lat)\n",
    "    va, vm = fit2Dsurf(x1, y1, p)\n",
    "    return va,vm\n",
    "\n",
    "\n",
    "def fit2Dsurf(x, y, p, kind='linear'):\n",
    "    \"\"\"\n",
    "    Get anomalies for a variable over a 2D map. Done by fitting a 2D surface \n",
    "    to the data and taking the anomaly as the difference between each data point \n",
    "    and the surface. Surface can either be a linear or quadratic function.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    x, y, p: 2D array-like, all same size.\n",
    "        Variables to use to fit the function p(x, y). x, y are the dependent vars\n",
    "        and p is the dependent var.\n",
    "    kind: str\n",
    "        (Default = 'linear'). Either 'linear' or 'quadratic' to specify the \n",
    "        functional form of the fit surface.\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    va, vm: 2D NumPy arrays\n",
    "        Anomalies (va) and mean surface fit (vm).\n",
    "\n",
    "    Import requirements\n",
    "    -------------------\n",
    "    from scipy.optimize import leastsq\n",
    "    numpy\n",
    "    \"\"\"\n",
    "    # Depending on fit fxn chosen, define functions to output a 2D surface (surface()) \n",
    "    # and the difference between 2D data and the computed surface (err()). Each \n",
    "    # fxn takes independent vars and polynomial coefficients; the err() fxn's in \n",
    "    # addition take data for the dependent var.\n",
    "    if kind=='linear':\n",
    "        def err(c,x0,y0,p):\n",
    "            a,b,c=c\n",
    "            return p - (a + b*x0 + c*y0 )\n",
    "\n",
    "        def surface(c,x0,y0):\n",
    "            a,b,c=c\n",
    "            return a + b*x0 + c*y0\n",
    "\n",
    "    if kind=='quadratic':\n",
    "        def err(c,x0,y0,p):\n",
    "            a,b,c,d,e,f=c\n",
    "            return p - (a + b*x0 + c*y0 + d*x0**2 + e*y0**2 + f*x0*y0)\n",
    "        \n",
    "        def surface(c,x0,y0):\n",
    "            a,b,c,d,e,f=c\n",
    "            return a + b*x0 + c*y0 + d*x0**2 + e*y0**2 + f*x0*y0\n",
    "\n",
    "\n",
    "    # Prep arrays and remove NAN's:\n",
    "    xf=x.flatten()\n",
    "    yf=y.flatten()\n",
    "    pf=p.flatten()\n",
    "\n",
    "    msk=~np.isnan(pf)\n",
    "    pf=pf[msk]\n",
    "    xf=xf[msk]\n",
    "    yf=yf[msk]\n",
    "\n",
    "    \n",
    "    # Initial values of polynomial coefficients to start fitting algorithm off with:\n",
    "    dpdx=(pf.max()-pf.min())/(xf.max()-xf.min())\n",
    "    dpdy=(pf.max()-pf.min())/(yf.max()-yf.min())\n",
    "    if kind=='linear':\n",
    "        c = [pf.mean(),dpdx,dpdy]\n",
    "    if kind=='quadratic':\n",
    "        c = [pf.mean(),dpdx,dpdy,1e-22,1e-22,1e-22]\n",
    "\n",
    "\n",
    "    # Fit:\n",
    "    coef = leastsq(err,c,args=(xf,yf,pf))[0]\n",
    "    vm = surface(coef,x,y) #mean surface\n",
    "    va = p - vm #anomaly\n",
    "    return va,vm\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    earthaccess.login()\n",
    "    granules_ssh = earthaccess.search_data(\n",
    "        short_name=\"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\",\n",
    "        temporal=(\"2010-01-01\", \"2011-12-31\"),\n",
    "        )\n",
    "    lats = np.arange(-80, 80, 2)\n",
    "    lons = np.arange(0, 359, 2)\n",
    "    coef, lats, lons = spatial_corrmap(granules_ssh[-1], 3, 3, lats=lats, lons=lons, f_notnull=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddea096-5680-4c47-88d6-2dd8e87c98f5",
   "metadata": {},
   "source": [
    "### Install the `line_profiler` tool and run it on the .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "912f7fd3-e47e-414c-8304-6340c953decc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: line_profiler in /opt/coiled/env/lib/python3.11/site-packages (4.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a63e3d-4664-43bd-b422-2514cf977bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 146\n",
      " Opening 1 granules, approx size: 0.01 GB\n",
      "using endpoint: https://archive.podaac.earthdata.nasa.gov/s3credentials\n",
      "QUEUEING TASKS | : 1it [00:00, 776.87it/s]\n",
      "PROCESSING TASKS | : 100%|████████████████████████| 1/1 [00:00<00:00,  4.62it/s]\n",
      "COLLECTING RESULTS | : 100%|███████████████████| 1/1 [00:00<00:00, 20971.52it/s]\n",
      "QUEUEING TASKS | : 1it [00:00, 2487.72it/s]\n",
      "PROCESSING TASKS | : 100%|████████████████████████| 1/1 [00:00<00:00, 66.34it/s]\n",
      "COLLECTING RESULTS | : 100%|███████████████████| 1/1 [00:00<00:00, 24105.20it/s]\n",
      "Wrote profile results to ssh_sst_correlation_test.py.lprof\n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 48.5762 s\n",
      "File: ssh_sst_correlation_test.py\n",
      "Function: spatial_corrmap at line 142\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   142                                           @profile\n",
      "   143                                           def spatial_corrmap(granule_ssh, lat_halfwin, lon_halfwin, lats=None, lons=None, f_notnull=0.5):\n",
      "   144                                               \"\"\"\n",
      "   145                                               Get a 2D map of SSH-SST spatial correlation coefficients. The SSH dataset is \n",
      "   146                                               shortname SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812 \n",
      "   147                                               and the SST dataset is shortname MW_OI-REMSS-L4-GLOB-v5.0. \n",
      "   148                                           \n",
      "   149                                               At each gridpoint, the spatial correlation is computed over a lat, lon window\n",
      "   150                                               of size 2*lat_halfwin x 2*lon_halfwin. This is done for each gridpoint in\n",
      "   151                                               the datasets. Spatial correlation is computed from the SSH, SST anomalies, \n",
      "   152                                               which are computed in turn as the deviations from a fitted 2D surface over \n",
      "   153                                               the window (a new 2D surface is fitted for each window around each gridpoint).\n",
      "   154                                               \n",
      "   155                                               Inputs\n",
      "   156                                               ------\n",
      "   157                                               granule_ssh: earthaccess.results.DataGranule\n",
      "   158                                                   Granule info for theSSH file, including download path and S3 location.  \n",
      "   159                                               lat_halfwin, lon_halfwin: floats\n",
      "   160                                                   Half window size in degrees for latitude and longitude dimensions, respectively.\n",
      "   161                                               lats, lons: None or 1D array-like\n",
      "   162                                                   These make up the latitude, longitude grid on which to compute correlations. \n",
      "   163                                                   If None, will default to using the grid of the SSH product. Note that regardless\n",
      "   164                                                   of the lats, lons passed to this function, it will still use the gridpoints \n",
      "   165                                                   of the SSH product for the actual computations.\n",
      "   166                                               f_notnull: float (default = 50)\n",
      "   167                                                   Fraction of elements in a window that have to be non-nan. Percentage is computed\n",
      "   168                                                   as number of null elements divided by total number expected to be in the window. So\n",
      "   169                                                   for edge cases, 'ghost' elements at the edges are considered nan.\n",
      "   170                                           \n",
      "   171                                               Returns\n",
      "   172                                               ------\n",
      "   173                                               coef: 2D numpy array\n",
      "   174                                                   Spatial correlation coefficients.\n",
      "   175                                               \n",
      "   176                                               lats, lons: 1D numpy arrays.\n",
      "   177                                                   Latitudes and longitudes creating the 2D grid that 'coef' was calculated on.\n",
      "   178                                               \"\"\"    \n",
      "   179                                               # Load datafiles, convert SST lon to (0,360) bounds, and interpolate SST to SSH grid:    \n",
      "   180         1    7125246.4    7e+06     14.7      ssh,sst = load_sst_ssh(granule_ssh)\n",
      "   181         1       1341.5   1341.5      0.0      sst = sst.roll(lon=len(sst['lon'])//2)\n",
      "   182         1      13323.8  13323.8      0.0      sst['lon'] = sst['lon']+180\n",
      "   183         1      67669.5  67669.5      0.1      sst = sst.interp(lon=ssh['Longitude']).interp(lat=ssh['Latitude'])\n",
      "   184                                           \n",
      "   185                                               \n",
      "   186                                               # Compute windows size and threshold number of non-nan points:\n",
      "   187         1       2225.3   2225.3      0.0      dlat = (ssh['Latitude'][1]-ssh['Latitude'][0]).item()\n",
      "   188         1       1662.2   1662.2      0.0      dlon = (ssh['Longitude'][1]-ssh['Longitude'][0]).item()\n",
      "   189         1          3.9      3.9      0.0      nx_win = 2*round(lon_halfwin/dlon)\n",
      "   190         1          0.5      0.5      0.0      ny_win = 2*round(lat_halfwin/dlat)\n",
      "   191         1          0.7      0.7      0.0      n_thresh = nx_win*ny_win*f_notnull\n",
      "   192                                           \n",
      "   193                                           \n",
      "   194                                               # Map of booleans for sst*ssh==np.nan. Will be used to determine if there are \n",
      "   195                                               # enough non-nan values to compute the correlation for a given window:\n",
      "   196         1       4898.3   4898.3      0.0      notnul = (sst*ssh).notnull()\n",
      "   197                                           \n",
      "   198                                               \n",
      "   199                                               # Compute spatial correlations over whole map:\n",
      "   200         1          0.3      0.3      0.0      coef = []\n",
      "   201                                               \n",
      "   202         1          0.5      0.5      0.0      if lats is None:\n",
      "   203                                                   lats = ssh['Latitude'].data\n",
      "   204                                                   lons = ssh['Longitude'].data\n",
      "   205                                               \n",
      "   206        81         83.5      1.0      0.0      for lat_cen in lats:\n",
      "   207     14480      11220.6      0.8      0.0          for lon_cen in lons:\n",
      "   208                                           \n",
      "   209                                                       # Create window for both sst and ssh with xr.sel:\n",
      "   210     14400       7916.3      0.5      0.0              lat_bottom = lat_cen - lat_halfwin\n",
      "   211     14400       5581.5      0.4      0.0              lat_top = lat_cen + lat_halfwin\n",
      "   212     14400       3764.5      0.3      0.0              lon_left = lon_cen - lon_halfwin\n",
      "   213     14400       3402.3      0.2      0.0              lon_right = lon_cen + lon_halfwin\n",
      "   214     14400    7095573.9    492.7     14.6              ssh_win = ssh.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
      "   215     14400    8710354.7    604.9     17.9              sst_win = sst.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
      "   216                                               \n",
      "   217                                                       # If number of non-nan values in sst*ssh window is less than threshold \n",
      "   218                                                       # value, append np.nan, else compute anomalies and append their correlation coefficient:\n",
      "   219     14400    8025523.6    557.3     16.5              notnul_win = notnul.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
      "   220     14400    1546470.8    107.4      3.2              n_notnul = notnul_win.sum().item()\n",
      "   221     14400       8504.1      0.6      0.0              if n_notnul < n_thresh:\n",
      "   222      6047       3991.3      0.7      0.0                  coef.append(np.nan)\n",
      "   223                                                       else:\n",
      "   224                                                           # Compute anomalies:\n",
      "   225      8353    6728379.3    805.5     13.9                  ssha,_=anomaly(ssh_win['Longitude'], ssh_win['Latitude'], ssh_win.data)\n",
      "   226      8353    6862901.2    821.6     14.1                  ssta,_=anomaly(sst_win['Longitude'], sst_win['Latitude'], sst_win.data)\n",
      "   227                                                           \n",
      "   228                                                           # Compute correlation coefficient:\n",
      "   229      8353      28261.7      3.4      0.1                  a, b = ssta.flatten(), ssha.flatten()\n",
      "   230      8353     368889.6     44.2      0.8                  if ( np.nansum(abs(a))==0 ) or ( np.nansum(abs(b))==0 ): # There are some cases where all anomalies for one var are 0.\n",
      "   231       130         89.7      0.7      0.0                      coef.append(0) # In this case, correlation should be 0. Numpy will compute this correctly, but will also throw a lot of warnings.\n",
      "   232                                                           else:\n",
      "   233      8223    1942081.9    236.2      4.0                      c = np.nanmean(a*b)/np.sqrt(np.nanvar(a) * np.nanvar(b))\n",
      "   234      8223       5519.4      0.7      0.0                      coef.append(c)\n",
      "   235                                                   \n",
      "   236                                                       \n",
      "   237         1       1338.2   1338.2      0.0      return np.array(coef).reshape((len(lats), len(lons))), np.array(lats), np.array(lons)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kernprof -lv ssh_sst_correlation_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d8861-d63d-476e-a138-310fae5cfaa4",
   "metadata": {},
   "source": [
    "### Line profiler results summarized\n",
    "* The three `.sel()` function calls (from the `Xarray` package) each take ~15% of the computation time, for a total of almost 50% of total comp time.\n",
    "* The two calls to `anomaly()` each take ~15% of the computation time, for a total of almost 30% of total comp time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4771ef46-87a2-4a59-86d5-5dabcbf8e822",
   "metadata": {},
   "source": [
    "## Testing if 3 .sel() calls on separate DataArrays is slower than combining all the arrays into one dataset then using one .sel() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9d2817-4f0f-4839-9f47-d23227aafa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 146\n"
     ]
    }
   ],
   "source": [
    "earthaccess.login()\n",
    "granules_ssh = earthaccess.search_data(\n",
    "    short_name=\"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\",\n",
    "    temporal=(\"2010-01-01\", \"2011-12-31\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8434264-60c2-4adc-9d10-61945b324164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Opening 1 granules, approx size: 0.01 GB\n",
      "using endpoint: https://archive.podaac.earthdata.nasa.gov/s3credentials\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bbfa46866d4d9296df640c90c66921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ed3ad4b5984d35babe48e2e8f8ded2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7237908905784592881db6356a355610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190eeffe0f7c42a0bca0aa40c3ff2ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5524de04e79404a9cdf5a23997e8d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88844ddcabaf4415bf14b18bef8c928d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ssh, sst = load_sst_ssh(granules_ssh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0704b22b-a3ae-497f-8839-9ad392e29ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = sst.roll(lon=len(sst['lon'])//2)\n",
    "sst['lon'] = sst['lon']+180\n",
    "sst = sst.interp(lon=ssh['Longitude']).interp(lat=ssh['Latitude'])\n",
    "\n",
    "# Map of booleans for sst*ssh==np.nan. Will be used to determine if there are \n",
    "# enough non-nan values to compute the correlation for a given window:\n",
    "notnul = (sst*ssh).notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2719a3b-a90d-4fab-8255-d3432da92810",
   "metadata": {},
   "outputs": [],
   "source": [
    "notnul = notnul.rename(\"notnul_sst*ssh\")\n",
    "merged = xr.merge([ssh, sst, notnul], compat=\"equals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7dd557a-7802-4c8a-be50-797ef7f2ad08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 ms, sys: 0 ns, total: 2.6 ms\n",
      "Wall time: 2.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ssh_win = ssh.sel(Longitude=slice(0, 6), Latitude=slice(-3, 3))\n",
    "sst_win = sst.sel(Longitude=slice(0, 6), Latitude=slice(-3, 3))\n",
    "notnul_win = notnul.sel(Longitude=slice(0, 6), Latitude=slice(-3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba1127b-418c-4f8c-9cc8-15c80b2246b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 720 µs, sys: 0 ns, total: 720 µs\n",
      "Wall time: 708 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "merged_win = merged.sel(Longitude=slice(0, 6), Latitude=slice(-3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6d0c0-d8dc-48fa-8a82-cffdc54124ad",
   "metadata": {},
   "source": [
    "Looks like we can cut computation time for the `.sel()` function calls by ~67% !\n",
    "Since these lines of code take ~50% of the running time for the entire function, this translates to an expected ~33% reduction in total computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb064fda-f531-4a79-a4be-7cbb879df955",
   "metadata": {},
   "source": [
    "## Revise `spatial_corrmap()` function and test for computation improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc6513a2-aaae-4b20-8c24-8a1731b8b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_corrmap_new(granule_ssh, lat_halfwin, lon_halfwin, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Get a 2D map of SSH-SST spatial correlation coefficients. The SSH dataset is \n",
    "    shortname SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812 \n",
    "    and the SST dataset is shortname MW_OI-REMSS-L4-GLOB-v5.0. \n",
    "\n",
    "    At each gridpoint, the spatial correlation is computed over a lat, lon window\n",
    "    of size 2*lat_halfwin x 2*lon_halfwin. This is done for each gridpoint in\n",
    "    the datasets. Spatial correlation is computed from the SSH, SST anomalies, \n",
    "    which are computed in turn as the deviations from a fitted 2D surface over \n",
    "    the window (a new 2D surface is fitted for each window around each gridpoint).\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    granule_ssh: earthaccess.results.DataGranule\n",
    "        Granule info for theSSH file, including download path and S3 location.  \n",
    "    lat_halfwin, lon_halfwin: floats\n",
    "        Half window size in degrees for latitude and longitude dimensions, respectively.\n",
    "    lats, lons: None or 1D array-like\n",
    "        These make up the latitude, longitude grid on which to compute correlations. \n",
    "        If None, will default to using the grid of the SSH product. Note that regardless\n",
    "        of the lats, lons passed to this function, it will still use the gridpoints \n",
    "        of the SSH product for the actual computations.\n",
    "    f_notnull: float (default = 50)\n",
    "        Fraction of elements in a window that have to be non-nan. Percentage is computed\n",
    "        as number of null elements divided by total number expected to be in the window. So\n",
    "        for edge cases, 'ghost' elements at the edges are considered nan.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    coef: 2D numpy array\n",
    "        Spatial correlation coefficients.\n",
    "    \n",
    "    lats, lons: 1D numpy arrays.\n",
    "        Latitudes and longitudes creating the 2D grid that 'coef' was calculated on.\n",
    "    \"\"\"    \n",
    "    # Load datafiles, convert SST longitude to (0,360), and interpolate SST to SSH grid:    \n",
    "    ssh,sst = load_sst_ssh(granule_ssh)\n",
    "    sst = sst.roll(lon=len(sst['lon'])//2)\n",
    "    sst['lon'] = sst['lon']+180\n",
    "    sst = sst.interp(lon=ssh['Longitude']).interp(lat=ssh['Latitude'])\n",
    "\n",
    "    \n",
    "    # Compute windows size and threshold number of non-nan points:\n",
    "    dlat = (ssh['Latitude'][1]-ssh['Latitude'][0]).item()\n",
    "    dlon = (ssh['Longitude'][1]-ssh['Longitude'][0]).item()\n",
    "    nx_win = 2*round(lon_halfwin/dlon)\n",
    "    ny_win = 2*round(lat_halfwin/dlat)\n",
    "    n_thresh = nx_win*ny_win*f_notnull\n",
    "\n",
    "\n",
    "    # Map of booleans for sst*ssh==np.nan. Will be used to determine if there are \n",
    "    # enough non-nan values to compute the correlation for a given window:\n",
    "    notnul = (sst*ssh).notnull()\n",
    "\n",
    "    \n",
    "    # Combine all needed DataArrays into a single Dataset for more efficient indexing:\n",
    "    ######################## Updated code ########################\n",
    "    notnul = notnul.rename(\"notnul\") # Needs a name to merge\n",
    "    mergeddata = xr.merge([ssh, sst, notnul], compat=\"equals\")\n",
    "    ######################## Updated code ########################\n",
    "\n",
    "    # Compute spatial correlations over whole map:\n",
    "    coef = []\n",
    "    \n",
    "    if lats is None:\n",
    "        lats = ssh['Latitude'].data\n",
    "        lons = ssh['Longitude'].data\n",
    "    \n",
    "    for lat_cen in lats:\n",
    "        for lon_cen in lons:\n",
    "\n",
    "            # Create window for both sst and ssh with xr.sel:\n",
    "            lat_bottom = lat_cen - lat_halfwin\n",
    "            lat_top = lat_cen + lat_halfwin\n",
    "            lon_left = lon_cen - lon_halfwin\n",
    "            lon_right = lon_cen + lon_halfwin\n",
    "            ######################## Updated code ########################\n",
    "            data_win = mergeddata.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "            ######################## Updated code ########################\n",
    "            \n",
    "            # If number of non-nan values in sst*ssh window is less than threshold \n",
    "            # value, append np.nan, else compute anomalies and append their correlation coefficient:\n",
    "            n_notnul = data_win[\"notnul\"].sum().item()\n",
    "            if n_notnul < n_thresh:\n",
    "                coef.append(np.nan)\n",
    "            else:\n",
    "                # Compute anomalies:\n",
    "                ######################## Updated code ########################\n",
    "                ssha,_=anomaly(data_win['Longitude'], data_win['Latitude'], data_win['SLA'].data)\n",
    "                ssta,_=anomaly(data_win['Longitude'], data_win['Latitude'], data_win['analysed_sst'].data)\n",
    "                ######################## Updated code ########################\n",
    "                \n",
    "                # Compute correlation coefficient:\n",
    "                a, b = ssta.flatten(), ssha.flatten()\n",
    "                if ( np.nansum(abs(a))==0 ) or ( np.nansum(abs(b))==0 ): # There are some cases where all anomalies for one var are 0.\n",
    "                    coef.append(0) # In this case, correlation should be 0. Numpy will compute this correctly, but will also throw a lot of warnings.\n",
    "                else:\n",
    "                    c = np.nanmean(a*b)/np.sqrt(np.nanvar(a) * np.nanvar(b))\n",
    "                    coef.append(c)\n",
    "        \n",
    "            \n",
    "    return np.array(coef).reshape((len(lats), len(lons))), np.array(lats), np.array(lons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e0b82-3482-4c25-adc3-bab71c8cf290",
   "metadata": {},
   "source": [
    "**Run old and new functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e97bace7-62ce-4a5a-8a8d-3374fb23d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.arange(-80, 80, 2)\n",
    "lons = np.arange(0, 359, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ee67d70-02e4-42b8-92b3-cc7626e7ef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Opening 1 granules, approx size: 0.01 GB\n",
      "using endpoint: https://archive.podaac.earthdata.nasa.gov/s3credentials\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb349eb9f7d345bfa76ab9dfde3aba57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa91ba666714e1f8c75ca010678d081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8b873777dc4ecf8267705aa3609142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb77add8114407ca2c63b98953ff609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5f4dd895f2402eb8596b845a67596b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12a248af24f488393bd44fc48adb042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 s, sys: 8.44 ms, total: 28.1 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "coef, lats, lons = spatial_corrmap(granules_ssh[-1], 3, 3, lats=lats, lons=lons, f_notnull=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a70b716-7432-4b8c-80e2-7dcf39a8e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Opening 1 granules, approx size: 0.01 GB\n",
      "using endpoint: https://archive.podaac.earthdata.nasa.gov/s3credentials\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4612683aa8034a7290d336272d191b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2babb8860264ce3b4c436bca50922c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be65023fb55e4222a90d9b2dc9342f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c2c029870146d695972abdaa762fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | : 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16d4e76418b411a89db5756c7262460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b885d01ed0417482f05108f42ef0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 s, sys: 53.4 ms, total: 20.3 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "coef, lats, lons = spatial_corrmap_new(granules_ssh[-1], 3, 3, lats=lats, lons=lons, f_notnull=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccb6f7-f680-4575-b346-a9d1117c0d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
