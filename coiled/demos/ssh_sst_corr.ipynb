{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "574f488f-29e0-48b2-9c7f-b9855ec19dea",
   "metadata": {},
   "source": [
    "# Parallel Computing with Coiled Function's\n",
    "## An Example Using the SST-SSH Spatial Correlation Analysis\n",
    "\n",
    "#### Authors: Dean Henze and Jinbo Wang, NASA JPL PO.DAAC\n",
    "*Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not constitute or imply its endorsement by the United States Government or the Jet Propulsion Laboratory, California Institute of Technology.*\n",
    "<br/><br/>\n",
    "\n",
    "Previous notebooks have covered the use of Dask and parallel computing applied to the type of task in the schematic below, where we wish to replicate a function identically over a set of files.\n",
    "\n",
    "<img src=\"./schematic1.png\" alt=\"sch1\" width=\"500\"/>\n",
    "\n",
    "Further, a previous notebook explored applying this technique to the case of computing global maps of spatial correlation between sea surface temperature (SST) and sea surface height (SSH). For a recap of this analysis, see [link to notebook] and the description further below here. In the previous notebook, we applied Dask to this problem using a local cluster and `dask.delayed()`. In this notebook, we perform the same computations, but this time parallelize them using the third party software/package `Coiled`. In short `Coiled` will allow us to summon AWS VM's and create a distributed cluster out of them, all with a few lines of Python from within this notebook. *This means, once your Coiled account is set up, you can run this notebook entirely from your laptop while the computations will be run on a distributed cluster in AWS.* For more information on Coiled, setting up an account, and connecting it to an AWS account, see their website https://www.coiled.io. \n",
    "\n",
    "\n",
    "#### SST-SSH Correlation Analysis\n",
    "\n",
    "This section briefly describes the analysis to which we will apply parallel computation. The analysis uses PO.DAAC hosted, gridded SSH and SST data sets:\n",
    "* MEaSUREs gridded SSH Version 2205: 0.17° x 0.17° resolution, global map, one file per 5-days, https://doi.org/10.5067/SLREF-CDRV3\n",
    "* GHRSST Level 4 MW_OI Global Foundation SST, V5.0: 0.25° x 0.25° resolution, global map, daily files, https://doi.org/10.5067/GHMWO-4FR05\n",
    "\n",
    "The time period of overlap between these data sets is 1998 – 2020, with 1808 days in total overlapping. For each pair SST, SSH files on these days, compute a map of spatial correlation between them, where the following method is used at each gridpoint:\n",
    "\n",
    "<img src=\"./schematic_sst-ssh_corr.png\" alt=\"sch_sst-ssh-corr\" width=\"1000\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff055bc-2f3e-4436-a413-1c6550641f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built in packages\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Math / science packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "# Plotting packages\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "# Cloud / parallel computing packages\n",
    "import earthaccess\n",
    "import coiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee13dd-275f-49f0-bc25-3d0fccde6b09",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "The main function implemented is `spatial_corrmap()`, which will return the map of correlations as a 2D array. The other functions written below are called by `spatial_corrmap()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4dd16-bfc9-4993-be7d-88f1cd747c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sst_ssh(gran_ssh, gran_sst):\n",
    "    \"\"\"\n",
    "    Return SLA and SST variables for a single file each of the \n",
    "    SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205 and MW_OI-REMSS-L4-GLOB-v5.0 \n",
    "    collections, respectively, returned as xarray.DataArray's. Input args are granule info \n",
    "    (earthaccess.results.DataGranule object's) for each collection.  \n",
    "    \"\"\"\n",
    "    earthaccess.login(strategy=\"environment\") # Confirm credentials are current\n",
    "    \n",
    "    # Get SLA and SST variables, loaded fully into local memory:\n",
    "    ssh = xr.load_dataset(earthaccess.open([gran_ssh], provider='POCLOUD')[0])['SLA'][0,...]\n",
    "    sst = xr.load_dataset(earthaccess.open([gran_sst], provider='POCLOUD')[0])['analysed_sst'][0,...]\n",
    "\n",
    "    return ssh, sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53cc76-82dc-4473-9d61-89be4618ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialcorr(x, y, p1, p2):\n",
    "    \"\"\"\n",
    "    Correlation between two 2D variables p1(x, y), p2(x, y), over the domain (x, y). Correlation is \n",
    "    computed between the anomalies of p1, p2, where anomalies for each variables are the deviations from \n",
    "    respective linear 2D surface fits.\n",
    "    \"\"\"\n",
    "    # Compute anomalies:\n",
    "    ssha, _ = anomalies_2Dsurf(x, y, p1, kind='linear') # See function further down.\n",
    "    ssta, _ = anomalies_2Dsurf(x, y, p2, kind='linear')\n",
    "    \n",
    "    # Compute correlation coefficient:\n",
    "    a, b = ssta.flatten(), ssha.flatten()\n",
    "    if ( np.nansum(abs(a))==0 ) or ( np.nansum(abs(b))==0 ): # There are some cases where all anomalies for one var are 0.\n",
    "        # In this case, correlation should be 0. Numpy will compute this correctly, but will also throw a lot of warnings.\n",
    "        # Get around this by manually appending 0 instead.\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nanmean(a*b)/np.sqrt(np.nanvar(a) * np.nanvar(b))\n",
    "\n",
    "\n",
    "def anomalies_2Dsurf(x, y, p, kind='linear'):\n",
    "    \"\"\"\n",
    "    Get anomalies for a variable over a 2D map. Done by fitting a 2D surface \n",
    "    to the data (scipy) and taking the anomaly as the difference between each data point \n",
    "    and the surface. Surface can either be a linear or quadratic function.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    x, y: 1D array-like.\n",
    "        Independent vars (likely the lon, lat coordinates).\n",
    "    p: 2D array-like, of shape (len(y), len(x)).\n",
    "        Dependent variable. 2D surface fit will be to p(x, y).\n",
    "    kind: str\n",
    "        Functional form of the fit surface. Either 'linear' (default) or 'quadratic'.\n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    va, vm: 2D NumPy arrays\n",
    "        Anomalies (va) and mean surface fit (vm).\n",
    "    \"\"\"\n",
    "    # Depending on fit fxn chosen, define functions to output a 2D surface (surface()) \n",
    "    # and the difference between 2D data and the computed surface (err()).\n",
    "    if kind=='linear':\n",
    "        def err(c,x0,y0,p): # Takes independent/dependent vars and poly coefficients\n",
    "            a,b,c=c\n",
    "            return p - (a + b*x0 + c*y0 )\n",
    "\n",
    "        def surface(c,x0,y0): # Takes independent vars and poly coefficients\n",
    "            a,b,c=c\n",
    "            return a + b*x0 + c*y0\n",
    "\n",
    "    if kind=='quadratic':\n",
    "        def err(c,x0,y0,p):\n",
    "            a,b,c,d,e,f=c\n",
    "            return p - (a + b*x0 + c*y0 + d*x0**2 + e*y0**2 + f*x0*y0)\n",
    "        \n",
    "        def surface(c,x0,y0):\n",
    "            a,b,c,d,e,f=c\n",
    "            return a + b*x0 + c*y0 + d*x0**2 + e*y0**2 + f*x0*y0\n",
    "\n",
    "\n",
    "    # Prep arrays and remove NAN's:\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    xf=xx.flatten()\n",
    "    yf=yy.flatten()\n",
    "    pf=p.flatten()\n",
    "\n",
    "    msk=~np.isnan(pf)\n",
    "    pf=pf[msk]\n",
    "    xf=xf[msk]\n",
    "    yf=yf[msk]\n",
    "\n",
    "    \n",
    "    # Initial values of polynomial coefficients to start fitting algorithm off with:\n",
    "    dpdx=(pf.max()-pf.min())/(xf.max()-xf.min())\n",
    "    dpdy=(pf.max()-pf.min())/(yf.max()-yf.min())\n",
    "    if kind=='linear':\n",
    "        c = [pf.mean(),dpdx,dpdy]\n",
    "    if kind=='quadratic':\n",
    "        c = [pf.mean(),dpdx,dpdy,1e-22,1e-22,1e-22]\n",
    "\n",
    "\n",
    "    # Fit and compute anomalies:\n",
    "    coef = leastsq(err,c,args=(xf,yf,pf))[0]\n",
    "    vm = surface(coef, xx, yy) # mean surface\n",
    "    va = p - vm # anomalies\n",
    "    return va, vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e23d8b-0c6a-4c39-943e-4c37b6cc0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_corrmap(grans, lat_halfwin, lon_halfwin, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Get a 2D map of SSH-SST spatial correlation coefficients, for one each of the \n",
    "    SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205 and MW_OI-REMSS-L4-GLOB-v5.0 collections. \n",
    "    At each gridpoint, the spatial correlation is computed over a lat, lon window of size \n",
    "    2*lat_halfwin x 2*lon_halfwin. Correlation is computed from the SSH, SST anomalies, which are computed \n",
    "    in turn as the deviations from a fitted 2D surface over the window.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    grans: 2-tuple of earthaccess.results.DataGranule objects\n",
    "        Granule info for the SSH, SST files (in that order). These objects contain https and S3 locations.\n",
    "    lat_halfwin, lon_halfwin: floats\n",
    "        Half window size in degrees for latitude and longitude dimensions, respectively.\n",
    "    lats, lons: None or 1D array-like\n",
    "        Latitude, longitude gridpoints at which to compute the correlations. \n",
    "        If None, will use the SSH grid.\n",
    "    f_notnull: float between 0-1 (default = 0.5)\n",
    "        Threshold fraction of non-NAN values in a window in order for the correlation to be computed,\n",
    "        otherwise NAN is returned for that grid point. For edge cases, 'ghost' elements are counted as NAN.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    coef: 2D numpy array\n",
    "        Spatial correlation coefficients.\n",
    "    \n",
    "    lats, lons: 1D numpy arrays.\n",
    "        Latitudes and longitudes creating the 2D grid that 'coef' was calculated on.\n",
    "    \"\"\"    \n",
    "    # Load datafiles, convert SST longitude to (0,360), and interpolate SST to SSH grid:    \n",
    "    ssh, sst = load_sst_ssh(*grans)\n",
    "    sst = sst.roll(lon=len(sst['lon'])//2)\n",
    "    sst['lon'] = sst['lon']+180\n",
    "    sst = sst.interp(lon=ssh['Longitude'], lat=ssh['Latitude'])\n",
    "\n",
    "    \n",
    "    # Compute windows size and threshold number of non-nan points:\n",
    "    dlat = (ssh['Latitude'][1]-ssh['Latitude'][0]).item()\n",
    "    dlon = (ssh['Longitude'][1]-ssh['Longitude'][0]).item()\n",
    "    nx_win = 2*round(lon_halfwin/dlon)\n",
    "    ny_win = 2*round(lat_halfwin/dlat)\n",
    "    n_thresh = nx_win*ny_win*f_notnull\n",
    "\n",
    "\n",
    "    # Some prep work for efficient identification of windows where number of non-nan's < threshold:\n",
    "        # Map of booleans for sst*ssh==np.nan\n",
    "    notnul = (sst*ssh).notnull() \n",
    "        # Combine map and sst, ssh data into single Dataset for more efficient indexing:\n",
    "    notnul = notnul.rename(\"notnul\") # Needs a name to merge\n",
    "    mergeddata = xr.merge([ssh, sst, notnul], compat=\"equals\")\n",
    "     \n",
    "\n",
    "    # Compute spatial correlations over whole map:\n",
    "    coef = []\n",
    "    \n",
    "    if lats is None:\n",
    "        lats = ssh['Latitude'].data\n",
    "        lons = ssh['Longitude'].data\n",
    "    \n",
    "    for lat_cen in lats:\n",
    "        for lon_cen in lons:\n",
    "\n",
    "            # Create window for both sst and ssh with xr.sel:\n",
    "            lat_bottom = lat_cen - lat_halfwin\n",
    "            lat_top = lat_cen + lat_halfwin\n",
    "            lon_left = lon_cen - lon_halfwin\n",
    "            lon_right = lon_cen + lon_halfwin\n",
    "            data_win = mergeddata.sel(Longitude=slice(lon_left, lon_right), Latitude=slice(lat_bottom, lat_top))\n",
    "    \n",
    "            # If number of non-nan values in window is less than threshold \n",
    "            # value, append np.nan, else compute correlation coefficient:\n",
    "            n_notnul = data_win[\"notnul\"].sum().item()\n",
    "            if n_notnul < n_thresh:\n",
    "                coef.append(np.nan)\n",
    "            else:\n",
    "                c = spatialcorr(data_win['Longitude'], data_win['Latitude'], data_win['SLA'].data, data_win['analysed_sst'].data)\n",
    "                coef.append(c)\n",
    "    \n",
    "    return np.array(coef).reshape((len(lats), len(lons))), np.array(lats), np.array(lons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1b430-56d1-412f-8c59-87128a4e45ed",
   "metadata": {},
   "source": [
    "# Get all matching pairs of SSH, SST granules for 2018\n",
    "\n",
    "The `spatial_corrmap()` function takes as one of its arguments a 2-tuple of `earthaccess.results.DataGranule` objects, one each for SSH and SST (recall that `earthaccess.results.DataGranule` objects are returned from a call to `earthaccess.search_data()`). This section will retrieve pairs of these objects for all SSH, SST data in 2018 on days where the data sets overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2069fe9-96cf-41df-97d1-c9e87f10caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64811a-9b86-403e-9dd5-10594219c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Granule info for all files in 2018:\n",
    "dt2018 = (\"2018-01-01\", \"2018-12-31\")\n",
    "grans_ssh = earthaccess.search_data(short_name=\"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\", temporal=dt2018)\n",
    "grans_sst = earthaccess.search_data(short_name=\"MW_OI-REMSS-L4-GLOB-v5.0\", temporal=dt2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc7a3d-e145-4793-afdf-a1197e6cfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## File coverage dates extracted from filenames:\n",
    "dates_ssh = [g['umm']['GranuleUR'].split('_')[-1][:8] for g in grans_ssh]\n",
    "dates_sst = [g['umm']['GranuleUR'][:8] for g in grans_sst]\n",
    "print(' SSH file days: ', dates_ssh[:8], '\\n', 'SST file days: ', dates_sst[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edfcfd-1898-47af-ac6b-c44722f3f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate granule info for dates where there are both SSH and SST files:\n",
    "grans_ssh_analyze = []\n",
    "grans_sst_analyze = []\n",
    "for j in range(len(dates_ssh)):\n",
    "    if dates_ssh[j] in dates_sst:\n",
    "        grans_ssh_analyze.append(grans_ssh[j])\n",
    "        grans_sst_analyze.append(grans_sst[dates_sst.index(dates_ssh[j])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ad58e-3720-4189-9678-c036f12f50ba",
   "metadata": {},
   "source": [
    "**The result is two lists of `earthaccess.results.DataGranule` objects, where the ith element of the SSH, SST lists contain granule info for the respective data sets on the same day:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad200075-c528-4943-bf94-a519ffe0cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grans_ssh_analyze[0]['umm']['CollectionReference']['ShortName'], ':', len(grans_ssh_analyze), 'granules')\n",
    "print([g['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'] for g in grans_ssh_analyze[:4]])\n",
    "print(grans_sst_analyze[0]['umm']['CollectionReference']['ShortName'], ':', len(grans_sst_analyze), 'granules')\n",
    "print([g['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'] for g in grans_sst_analyze[:4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3636ae-d8af-4d64-9bd1-d80146a54917",
   "metadata": {},
   "source": [
    "# Test the computation on a pair of files, output on a coarse resolution grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03e8f0-bea8-430d-8b1d-fa156661cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spatial correlation map for 2 degree x 2 degree resolution and time it:\n",
    "t1 = time.time()\n",
    "\n",
    "lats = np.arange(-80, 80, 2)\n",
    "lons = np.arange(0, 359, 2)\n",
    "coef, lats, lons = spatial_corrmap((grans_ssh_analyze[0], grans_sst_analyze[0]), 3, 3, lats=lats, lons=lons, f_notnull=0.5)\n",
    "\n",
    "t2 = time.time()\n",
    "comptime = round(t2-t1, 2)\n",
    "print(\"Total computation time = \" + str(comptime) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeedc7b-446f-475c-9e74-3d968ccecbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the results:\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.contourf(lons, lats, coef, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfaabf5-4e9a-47c0-a3cc-2c3cd1e6b449",
   "metadata": {},
   "source": [
    "### Estimation of computation time for higher resolution output and more files\n",
    "\n",
    "The computation for one file computed on a 2 x 2 degree grid takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa50edd-81f7-4965-b3c3-b59aacfe8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(comptime) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493cc0c-e571-489a-b2da-7a959f59ad56",
   "metadata": {},
   "source": [
    "then assuming linear scaling, processing one file at 0.5 x 0.5 degree resolution would take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8e45b-d352-4c16-9647-6796cddc1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_fullres_seconds = comptime*(2/0.5)*(2/0.5)\n",
    "eta_fullres_minutes = round(eta_fullres_seconds/60)\n",
    "print(str(eta_fullres_minutes) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45afc1-86e4-402c-a0a1-2c58d5c82c0c",
   "metadata": {},
   "source": [
    "and for the record over all of 2018 would take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339cce06-7228-45ce-b254-8f2c611b5581",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_allfiles_hrs = round( (len(grans_ssh)*eta_fullres_minutes)/60, 1 )\n",
    "eta_allfiles_days = round(eta_allfiles_hrs/24, 2)\n",
    "print(str(len(grans_ssh)) + \" granules for 2018.\")\n",
    "print(str(eta_allfiles_hrs) + \" hours = \" + str(eta_allfiles_days) + \" days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f37e27c-b2c1-4682-9902-04457dd69f71",
   "metadata": {},
   "source": [
    "# Parallel computations with Coiled Functions\n",
    "The previous section showed that analyzing a year's worth of data at 0.5 x 0.5 degree output resolution would take 7-8 hrs (at least at the time this notebook was written, perhaps it will be shorter in the future?). In this section, we use Coiled to parallelize this computation to take ~10 minutes and cost ~$1 (again, at the time this was written). Recall that our task is to replicate and apply our function to all the files in 2018. This type of parallelization can be accomplished with a call to `coiled.function()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c33ba-45c5-4089-af3d-6b4b4bfbc282",
   "metadata": {},
   "source": [
    "First, we define a wrapper function which calls `spatial_corrmap()` for a pair of SSH, SST granules and collects the output into an Xarray DataArray, returning it along with the date of granule coverages. We will parallelize this function rather than `spatial_corrmap()`, as it will let us easily take the output and save to netCDF files locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3df8d-56f6-44bf-87fc-4efa2098866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrmap_toda(grans, lat_halfwin=3, lon_halfwin=3, lats=None, lons=None, f_notnull=0.5):\n",
    "    \"\"\"\n",
    "    Calls spatial_corrmap() for a pair of SSH, SST granules and collects output into an Xarray DataArray. \n",
    "    Returns this along with the date of the file coverages.\n",
    "    \"\"\"\n",
    "    coef, lats, lons = spatial_corrmap(grans, lat_halfwin, lon_halfwin, lats=lats, lons=lons, f_notnull=0.5)\n",
    "    date = grans[0]['umm']['GranuleUR'].split(\"_\")[-1][:8] # get date from SSH UR.\n",
    "    return date, xr.DataArray(data=coef, dims=[\"lat\", \"lon\"], coords=dict(lon=lons, lat=lats), name='corr_ssh_sst')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb257b8-01c0-4c20-b5b8-6e12b82108af",
   "metadata": {},
   "source": [
    "Next, do a little prep work, then setup the parallel computations and run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e6584-dd66-4955-a00e-407ebc42e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All output will be saved to this local directory:\n",
    "dir_results = \"results/\"\n",
    "os.makedirs(dir_results, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b39637-780a-47fc-bcec-98b1f772f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latitudes, longitudes of output grid at 0.5 degree resolution:\n",
    "lats = np.arange(-80, 80, 0.5)\n",
    "lons = np.arange(0, 359, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39015d-1eb9-4203-af85-28e3b5a47987",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -----------------------------\n",
    "## Perform and time computations\n",
    "## -----------------------------\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "# Wrap function in a Coiled function. Here is where we make cluster specifications like VM type:\n",
    "corrmap_toda_parallel = coiled.function(\n",
    "    region=\"us-west-2\", spot_policy=\"on-demand\", vm_type=\"m6i.large\", \n",
    "    environ=earthaccess.auth_environ() # This ensures that our EDL credentials are passed to each worker.\n",
    "    )(corrmap_toda)\n",
    "\n",
    "# Optional: manually scale workers:\n",
    "corrmap_toda_parallel.cluster.scale(73)\n",
    "\n",
    "# Begin computations:\n",
    "grans_2tuples = list(zip(grans_ssh_analyze, grans_sst_analyze))\n",
    "results = corrmap_toda_parallel.map(grans_2tuples, lat_halfwin=3, lon_halfwin=3, lats=lats, lons=lons, f_notnull=0.5)\n",
    "\n",
    "# Retreive the results from the cluster as they become available and save as .nc files locally:\n",
    "for date, result_da in results:\n",
    "     result_da.to_netcdf(dir_results+\"spatial-corr-map_ssh-sst_\" + date + \".nc\")\n",
    "\n",
    "# Since we manually scaled up the cluster, we have to manually scale it back down if we want to stop using resources:\n",
    "corrmap_toda_parallel.cluster.scale(1)\n",
    "\n",
    "\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9f9e1-2aef-450e-a438-96d6d5a32213",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What was the total computation time?\n",
    "comptime = round(t2-t1, 2)\n",
    "print(\"Total computation time = \" + str(comptime) + \" seconds = \" + str(comptime/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc53dc-719d-4540-a177-5c9af42b0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmap_toda_parallel.cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac4c67-832c-438c-892b-1f997b82720a",
   "metadata": {},
   "source": [
    "## Open and plot results from one of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae51e32-9a61-4e82-9c56-902a493fa4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns_results = [f for f in os.listdir(dir_results) if f.endswith(\"nc\")]\n",
    "testfile = xr.load_dataset(dir_results + fns_results[-1])\n",
    "testfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add500ba-72f4-4abc-8032-4ef60941ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile[\"corr_ssh_sst\"].plot(figsize=(8,3), vmin=-1, vmax=1, cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6adb0f6-c7a2-4f3e-9ca3-2d67a62d406b",
   "metadata": {},
   "source": [
    "## Other notes\n",
    "\n",
    "1. When using `coiled.function()`, we called `corrmap_toda_parallel.cluster.scale(73)` because we knew we wanted 73 workers to process the 73 files. However, this line isn't mandatory - if we left it out, `Coiled` would have started with 1 worker and then \"adaptively scaled\", figuring out that it needed 73 workers to do the job faster. It takes a few extra minutes for `Coiled` to figure out exactly how many workers to scale up to, so we short-cutted the process here, since a few minutes in demo land is a lifetime.\n",
    "2. When we made the call to `coiled.function()` we chose input args which would prioritize computation time over cost. Alternate choices could be made to prioritize cost instead, e.g. the following should be less expensive:\n",
    "```\n",
    "coiled.function(region=\"us-west-2\", cpu=1, arm=True, spot_policy=\"spot_with_fallback\", environ=earthaccess.auth_environ())(corrmap_toda)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbc210-ffe6-4d0f-b492-5b26f9fd4a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
