{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca35470-26ba-4e06-bc4b-0670e003b2c1",
   "metadata": {},
   "source": [
    "# Parallel Computing with Earthdata in the Cloud\n",
    "## Processing a Large Data Set in Chunks Using `coiled.cluster()`, Example Use for an SST-SSH Spatial Correlation Analysis\n",
    "\n",
    "#### *Authors: Dean Henze, NASA JPL PO.DAAC*\n",
    "\n",
    "*Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not constitute or imply its endorsement by the United States Government or the Jet Propulsion Laboratory, California Institute of Technology.*\n",
    "\n",
    "## Summary\n",
    "\n",
    "\n",
    "Previous notebooks have covered the use of Dask and parallel computing applied to the type of tasks in the schematic below, where we have a function which needs to work on a large data set as a whole. This could e.g. because the function works on some or all of the data from each of the files, so we can't just work on each file independently like in the function replication example.\n",
    "\n",
    "<img src=\"./schematic3.png\" alt=\"sch1\" width=\"500\"/>\n",
    "<img src=\"./schematic2.png\" alt=\"sch1\" width=\"500\"/>\n",
    "\n",
    "In a previous notebook, a toy example was used to demonstrate this basic functionality using a local dask cluster and Xarray built-in functions to work on the data set in chunks. In this notebook, we expand that workflow to a more complex analysis, representing something closer to a real-world use-case. In this notebook, we parallelize computations using the third party software/package `Coiled`. In short, `Coiled` will allow us to spin up AWS virtual machines (EC2 instances) and create a distributed cluster out of them, all with a few lines of Python from within this notebook. *You will need a Coiled account, but once set up, you can run this notebook entirely from your laptop while the parallel computation portion will be run on the distributed cluster in AWS.* \n",
    "\n",
    "\n",
    "#### Analysis: Mean Seasonal Cycle of SST Anomalies\n",
    "\n",
    "The analysis will generate the mean seasonal cycle of sea surface temperature (SST) at each gridpoint in a region of the west coast of the U.S.A. \n",
    "The analysis uses a PO.DAAC hosted gridded global SST data sets:\n",
    "* GHRSST Level 4 MUR Global Foundation SST Analysis, V4.1: 0.01° x 0.01° resolution, global map, daily files, https://doi.org/10.5067/GHGMR-4FJ04\n",
    "\n",
    "The analysis will use files over the first decade of the time record. The files will be thinned out to once per week for the purposes of this notebook, but uncompressed the data will still be ~1.3 TB in memory. The following procedure is used to generate seasonal cycles:\n",
    "\n",
    "<img src=\"./schematic_sst-cycle.png\" alt=\"sch_sst-ssh-corr\" width=\"800\"/>\n",
    "\n",
    "\n",
    "# ***!!!!Summarize section in this notebook!!!!***\n",
    "\n",
    "\n",
    "## Requirements, prerequisite knowledge, learning outcomes\n",
    "\n",
    "#### Requirements to run this notebook\n",
    "* **Earthdata login account:** An Earthdata Login account is required to access data from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. \n",
    "* **Coiled account:** Create a coiled account (free to sign up), and connect it to an AWS account. For more information on Coiled, setting up an account, and connecting it to an AWS account, see their website https://www.coiled.io. \n",
    "* **Compute environment:** This notebook can be run either in the cloud (AWS instance running in us-west-2), or on a local compute environment (e.g. laptop, server), but the data loading step currently works substantially faster in the cloud. In both cases, the parallel computations are still sent to VM's in the cloud.\n",
    "\n",
    "\n",
    "#### Prerequisite knowledge\n",
    "* The [notebook on Dask basics](https://podaac.github.io/tutorials/notebooks/Advanced_cloud/basic_dask.html) and all prerequisites therein.\n",
    "\n",
    "#### Learning outcomes\n",
    "This notebook demonstrates how to use Coiled with a distributed cluster to replicate a function over many files in parallel. You will get better insight on how to apply this workflow to your own analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d2887-7f22-4610-8bd4-d04fe15083e2",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We ran this notebook in a Python 3.12.3 environment. The minimal working install we used to run this notebook from a clean environment was:\n",
    "\n",
    "*With pip:*\n",
    "\n",
    "```\n",
    "pip install xarray==2024.1.0 numpy==1.26.3 h5netcdf==1.3.0 \"dask[complete]\"==2024.5.2 earthaccess==0.9.0 matplotlib==3.8.0 coiled==1.28.0 jupyterlab\n",
    "```\n",
    "\n",
    "*or with conda:*\n",
    "\n",
    "```\n",
    "conda install -c conda-forge xarray==2024.1.0 numpy==1.26.3 h5netcdf==1.3.0 dask==2024.5.2 earthaccess==0.9.0 matplotlib==3.8.0 coiled==1.28.0 jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d6b3ba-bdcd-4153-af18-86594768e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version numbers listed next to each package:\n",
    "\n",
    "# Data location and access packages:\n",
    "import earthaccess                                 # 0.6.1\n",
    "\n",
    "# Analysis packages:\n",
    "import xarray as xr                                # 2023.9.0\n",
    "import numpy as np                                 # 1.26.0\n",
    "\n",
    "# Visualization packages:\n",
    "import matplotlib.pyplot as plt                    # 3.8.0\n",
    "%matplotlib inline\n",
    "\n",
    "# Cloud computing / dask packages:\n",
    "import coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca49d37-ea6f-4873-8a85-0116b46c0a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x12fbd0a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.set_options( # display options for xarray objects\n",
    "    display_expand_attrs=False,\n",
    "    display_expand_coords=True,\n",
    "    display_expand_data=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a80e30-93e8-4078-a385-7b8bfe99a59a",
   "metadata": {},
   "source": [
    "# Earthdata Login and locate MUR file access endpoints for first decade of record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a287817-162c-4036-850b-f70f6843861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<earthaccess.auth.Auth at 0x106322590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthaccess.login() # Login with your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa9b57-7ab3-4918-a4e9-e64d38700e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "datainfo = earthaccess.search_data(\n",
    "    short_name=\"MUR-JPL-L4-GLOB-v4.1\",\n",
    "    cloud_hosted=True,\n",
    "    temporal=(\"2002-01-01\", \"2013-05-01\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc85ec-735f-4ab9-ac7f-5f6c0572f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datainfo[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4644c-683c-46fb-abf1-0dc911104148",
   "metadata": {},
   "source": [
    "# Inspect a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa56a14-307b-4298-adbd-d313335cc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open a file:\n",
    "fileobj_test = earthaccess.open([datainfo[0]])[0] # Generate file objects from the endpoints which are compatible with Xarray\n",
    "sst_test = xr.open_dataset(fileobj_test)['analysed_sst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f673f26-19dc-4a7c-a524-517bf7e7e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define some geolocations for the analysis:\n",
    "\n",
    "# Region to perform analysis over:\n",
    "lat_region = (30, 45)\n",
    "lon_region = (-135, -105)\n",
    "\n",
    "# Points to plot seasonal cycle at:\n",
    "lat_points = (38, 38, 38, 38)\n",
    "lon_points = (-123.25, -125, -128, -132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12258854-db0a-45ae-bbcb-c7e8d9ea3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot SST in analysis region and mark 4 points for seasonal cycle:\n",
    "fig = plt.figure()\n",
    "sst_test.sel(lat=slice(*lat_region), lon=slice(*lon_region)).plot(cmap='RdYlBu_r')\n",
    "\n",
    "for lat, lon in zip(lat_points, lon_points):\n",
    "    plt.scatter(lon, lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9902b32-3351-4544-b056-61445d37967f",
   "metadata": {},
   "source": [
    "# Compute mean seasonal cycle for a decade of data at weekly temporal resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe1018-8db6-4321-a533-c42452a6a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thin out data files to get weekly temporal resolution for this demo:\n",
    "datainfo_thinned = [datainfo[i] for i in range(len(datainfo)) if i%7==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd312de-5fe3-4144-af4d-d2e1860a1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm we have about a decade of files and at weekly resolution:\n",
    "print(\"First and last file times \\n--------------------------\")\n",
    "print(datainfo_thinned[0]['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'])\n",
    "print(datainfo_thinned[-1]['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'])\n",
    "print(\"\\nFirst and second file times \\n--------------------------\")\n",
    "print(datainfo_thinned[0]['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'])\n",
    "print(datainfo_thinned[1]['umm']['TemporalExtent']['RangeDateTime']['BeginningDateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658a41d-5d3a-4e90-bd2f-134a9f08aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=25, \n",
    "    account=\"podaac-science\", \n",
    "    region=\"us-west-2\", \n",
    "    worker_vm_types=\"c7g.large\", # or can try \"m7a.medium\"\n",
    "    scheduler_vm_types=\"c7g.large\" # or can try \"m7a.medium\"\n",
    "    #name=''\n",
    "    ) \n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8c70f-eadf-438a-98a3-5d24560ec11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fileobjs = earthaccess.open(datainfo_thinned) # Generate file objects from the endpoints which are compatible with Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2366bc-4be1-4379-97cd-209488ea5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Load files and rechunk SST data:\n",
    "murdata = xr.open_mfdataset(fileobjs, parallel=True, chunks={'lat': 6000, 'lon': 6000, 'time': 1})\n",
    "sst = murdata[\"analysed_sst\"]\n",
    "sst = sst.chunk(chunks={'lat': 500, 'lon': 500, 'time': 200})\n",
    "sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb99a00-1077-473f-bb3c-02425b87dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------\n",
    "## Set up analysis\n",
    "## ----------------\n",
    "## (Since we're dealing with dask arrays, these functions calls don't do the computations yet, just set them up)\n",
    "\n",
    "## Subset to region off U.S.A. west coast:\n",
    "sst_regional = sst.sel(lat=slice(*lat_region), lon=slice(*lon_region))\n",
    "\n",
    "## Remove linear warming trend:\n",
    "p = sst_regional.polyfit(dim='time', deg=1) # Degree 1 polynomial fit coefficients over time for each lat, lon.\n",
    "fit = xr.polyval(sst_regional['time'], p.polyfit_coefficients) # Compute linear trend time series at each lat, lon.\n",
    "sst_detrend = (sst_regional - fit) # xarray is smart enough to subtract along the time dim only.\n",
    "\n",
    "## Mean seasonal cycle:\n",
    "seasonal_cycle = sst_detrend.groupby(\"time.month\").mean(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593c868-28de-478e-806c-97eae18d8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## ----------------\n",
    "## Compute it all!!\n",
    "## ----------------\n",
    "seasonal_cycle = seasonal_cycle.compute()\n",
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3f73f-39bf-4b36-8649-d14d99223730",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39534207-9d12-47e3-ba0d-3a986b9512c3",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48056892-de37-44a2-a528-6bad4359fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axes2 = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "## Replot the map and points from the test file:\n",
    "sst_test.sel(lat=slice(*lat_region), lon=slice(*lon_region)).plot(ax=axes2[0], cmap='RdYlBu_r')\n",
    "for lat, lon in zip(lat_points, lon_points):\n",
    "    axes2[0].scatter(lon, lat)\n",
    "\n",
    "## Seasonal cycles on another plot\n",
    "for lat, lon in zip(lat_points, lon_points):\n",
    "    scycle_point = seasonal_cycle.sel(lat=lat, lon=lon)\n",
    "    axes2[1].plot(scycle_point['month'], scycle_point.values, 'o-')\n",
    "\n",
    "axes2[1].set_title(\"Seasonal cycle of temperature anomalies \\n at four test points\", fontsize=14)\n",
    "axes2[1].set_xlabel(\"month\", fontsize=12)\n",
    "axes2[1].set_ylabel(r\"$\\Delta$T (K)\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f5a1d-6b75-4d85-b6f3-1ea688688543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
